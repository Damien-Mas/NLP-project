{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e5dd38",
   "metadata": {},
   "source": [
    "# ALL OF THIS CODE WAS COPIED FROM LECTURE 2.2. I HAVE ONLY ADDED A FEW COMMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d003ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell imports all the correct libraries to run the code successfully.\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "608941ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(681284, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This cell loads the corpus which comes as a CSV file, into a pandas dataframe. It was copied from lecture 2.2\n",
    "\n",
    "blog_df = pd.read_csv('data/blogtext.csv', encoding='utf-8')\n",
    "blog_df.shape #<--This part shows how many words are contained in the entire document and how many columns the \n",
    "              #CSV file contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a0a09ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the tokeniser, as explained in the critical report. This code was copied from lecture 2.2.\n",
    "\n",
    "def my_tokeniser(doc):\n",
    "    #Split on spaces\n",
    "    tokens = re.split(r'[-\\s.,;!?]+', doc)\n",
    "    processed = []\n",
    "    for t in tokens:\n",
    "        #Lemmatise and make lowercase\n",
    "        t = lem.lemmatize(t.lower())\n",
    "        #Remove stop words\n",
    "        if not t in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            processed = processed + [t]\n",
    "    #Return an array of tokens for that document\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6611f573",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c468b2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 83526)\n"
     ]
    }
   ],
   "source": [
    "# Your code here (feel free to add cells)\n",
    "#We calculate LDA on the Bag Of Words, NOT TFIDF\n",
    "count_vectoriser = CountVectorizer(tokenizer=my_tokeniser)\n",
    "bag_of_words = count_vectoriser.fit_transform(blog_df.sample(10000)[\"text\"]) # <-This takes a maximum of\n",
    "vocab = count_vectoriser.get_feature_names_out()                               #20000 documents otherwise my laptop\n",
    "                                                                               #crashes\n",
    "print(bag_of_words.todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10305d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many topics?\n",
    "num_topics = 14                          #<-- This chooses the number of topics\n",
    "pd.options.display.max_columns=num_topics\n",
    "labels = ['topic{}'.format(i) for i in range(num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e2cccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=num_topics,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10471983",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics = lda.fit_transform(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5820b11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0:\n",
      "[')' 'woman' 'ha' 'year' 'god' 'penny' 'look' 'hand' \"it's\" 'html' 'says:'\n",
      " 'game' 'team' 'like' 'new' 'wa' '' 'com/' \"'\" 'http://www']\n",
      "topic 1:\n",
      "['>' 'yeah' 'say' 'abortion' 'love' '|' 'kayltcfan' '::' ')' 'ha' 'like'\n",
      " 'u' '*me*' '*love*' '/' \"'\" '' '*' 'h0lla' ':']\n",
      "topic 2:\n",
      "['(' '5' 'ha' 'yes' 'jumper991:' 'make' 'like' 'si' \"'\" ')' 'ako' 'game'\n",
      " 'ang' 'ng' ']' 'sa' 'ko' 'na' '' '&nbsp']\n",
      "topic 3:\n",
      "['email' 'free' '2' 'new' 'use' 'el' 'game' 'internet' 'window' 'software'\n",
      " 'file' 'ha' 'e' 'urllink' 'y' 'la' 'server' 'que' 'computer' '']\n",
      "topic 4:\n",
      "['say' 'make' 'want' 'people' 'got' 'going' 'good' 'day' 'thing' \"don't\"\n",
      " 'think' \"it's\" 'really' 'time' 'know' \"i'm\" 'like' 'just' '' 'wa']\n",
      "topic 5:\n",
      "['thou' ')' 'loupette:' 'evil' 'ja' 'ma' 'die' 'wa' 'church' 'play'\n",
      " 'mission' 'album' 'guitar' 'ha' 'und' 'quotejill:' 'ich' 'quotejoel:' ''\n",
      " 'song']\n",
      "topic 6:\n",
      "['president' 'child' 'time' 'iraq' 'government' 'country' 'said' 'year'\n",
      " 'bush' 'american' 'right' 'war' 'world' '' 'state' 'u' \"'\" 'wa' 'ha'\n",
      " 'people']\n",
      "topic 7:\n",
      "['make' \"it's\" 'check' 'page' 'read' 'post' 'time' 'book' 'wa' 'com'\n",
      " 'like' 'just' 'link' 'ha' 'site' 'new' 'blog' \"'\" 'urllink' '']\n",
      "topic 8:\n",
      "['eye' 'day' 'arianna' 'jennifernycity' 'amsd1980' 'just' 'told' 'way'\n",
      " 'man' 'did' '”' 'went' 'asked' 'like' 'pm):' 'u' 'said' '' \"'\" 'wa']\n",
      "topic 9:\n",
      "['group' '–' 'sector' 'leader' 'social' 'christian' 'resident' 'new'\n",
      " 'condi:' 'george:' 'muslim' 'development' 'bual' 'conflict' 'project'\n",
      " 'building' 'program' 'la' 'community' 'peace']\n",
      "topic 10:\n",
      "['today' 'thing' 'ha' 'night' 'came' 'week' \"'\" 'like' '3' 'good' 'home'\n",
      " 'did' '2' 'year' 'got' 'time' 'day' 'went' '' 'wa']\n",
      "topic 11:\n",
      "['make' 'water' 'c' 'e' 'year' 'right' '4' 'time' 'u' '5' 'god' 'ha' '2'\n",
      " '3' '1' 'like' 'new' 'urllink' 'wa' '']\n",
      "topic 12:\n",
      "['urllink' 'le' 'post' 'dan' '1' 'wolf' 'superman' 'ni' 'ai' '01' '2' 'la'\n",
      " 'woman' 'wrath' 'wo' 'ha' 'word' 'pm):' 'aku' '']\n",
      "topic 13:\n",
      "['dat' 'love' 'ppl' 'mr' 'lol' 'wat' 'tt' 'dun' '2' 'said' 'den' 'wa' 'im'\n",
      " 'haha' 'ksylverio:' 'suecutey:' \"'\" 'u' 'n' '']\n"
     ]
    }
   ],
   "source": [
    "#Most relevant tokens for each topic\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    print(\"topic \" + str(i) + \":\")\n",
    "    #Get last n tokens (highest values)\n",
    "    print(vocab[topic.argsort()[-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35395d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
