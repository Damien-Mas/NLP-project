{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250373d4",
   "metadata": {},
   "source": [
    "# MOST OF THIS CODE WAS COPIED FROM LECTURE 2.2. THE PIECES OF CODE WHICH HAVE NOT BEEN COPIED WILL BE POINTED OUT AS PART OF THE COMMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95fc188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell imports all the correct libraries to run the code successfully\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f94cc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(681284, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This cell loads the corpus which comes as a CSV file, into a pandas dataframe. It was copied from lecture 2.2\n",
    "\n",
    "blog_df = pd.read_csv('data/blogtext.csv', encoding='utf-8')\n",
    "blog_df.shape #<--This part shows how many words are contained in the entire document and how many columns the \n",
    "              #CSV file contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c179a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>I had an interesting conversation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Somehow Coca-Cola has a way of su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>If anything, Korea is a country o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Take a read of this news article ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>I surf the English news sites a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>Ah, the Korean language...it look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>If you click on my profile you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>Last night was pretty fun...mostl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>There is so much that is differen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>urlLink    Here it is, the super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>One thing I love about Seoul (and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>urlLink    Wonderful oh-gyup-sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>18,June,2004</td>\n",
       "      <td>Here is the latest from the Korea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>17,June,2004</td>\n",
       "      <td>Well, I stand corrected, again.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>16,June,2004</td>\n",
       "      <td>So I've been in Vancouver a few d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id gender  age              topic      sign          date  \\\n",
       "0   2059027   male   15            Student       Leo   14,May,2004   \n",
       "1   2059027   male   15            Student       Leo   13,May,2004   \n",
       "2   2059027   male   15            Student       Leo   12,May,2004   \n",
       "3   2059027   male   15            Student       Leo   12,May,2004   \n",
       "4   3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "5   3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "6   3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "7   3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "8   3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "9   3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "10  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "11  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "12  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "13  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "14  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "15  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "16  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "17  3581210   male   33  InvestmentBanking  Aquarius  18,June,2004   \n",
       "18  3581210   male   33  InvestmentBanking  Aquarius  17,June,2004   \n",
       "19  3581210   male   33  InvestmentBanking  Aquarius  16,June,2004   \n",
       "\n",
       "                                                 text  \n",
       "0              Info has been found (+/- 100 pages,...  \n",
       "1              These are the team members:   Drewe...  \n",
       "2              In het kader van kernfusie op aarde...  \n",
       "3                    testing!!!  testing!!!            \n",
       "4                Thanks to Yahoo!'s Toolbar I can ...  \n",
       "5                I had an interesting conversation...  \n",
       "6                Somehow Coca-Cola has a way of su...  \n",
       "7                If anything, Korea is a country o...  \n",
       "8                Take a read of this news article ...  \n",
       "9                I surf the English news sites a l...  \n",
       "10               Ah, the Korean language...it look...  \n",
       "11               If you click on my profile you'll...  \n",
       "12               Last night was pretty fun...mostl...  \n",
       "13               There is so much that is differen...  \n",
       "14                urlLink    Here it is, the super...  \n",
       "15               One thing I love about Seoul (and...  \n",
       "16                urlLink    Wonderful oh-gyup-sal...  \n",
       "17               Here is the latest from the Korea...  \n",
       "18               Well, I stand corrected, again.  ...  \n",
       "19               So I've been in Vancouver a few d...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This cell provides a sample of the first 20 entries in the pandas dataframe\n",
    "\n",
    "blog_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25bae07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the tokeniser, as explained in the critical report. This code was copied from lecture 2.2.\n",
    "\n",
    "def my_tokeniser(doc):\n",
    "    #Split on spaces\n",
    "    tokens = re.split(r'[-\\s.,;!?]+', doc)\n",
    "    processed = []\n",
    "    for t in tokens:\n",
    "        #Lemmatise and make lowercase\n",
    "        t = lem.lemmatize(t.lower())\n",
    "        #Remove stop words\n",
    "        if not t in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            processed = processed + [t]\n",
    "    #Return an array of tokens for that document\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd49717",
   "metadata": {},
   "source": [
    "# Zodiac "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e44386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wrote the code in this cell myself. As explained in the critical report this takes a slice of the corpus\n",
    "#by star sign to analyse. It's a bit clunky but it works.\n",
    "\n",
    "aquarius_text = [] #<-- The entries for the aquarius star sign are loaded in this list. The same is repeated\n",
    "aries_text = []    #for every star sign.\n",
    "cancer_text = []\n",
    "capricorn_text = []\n",
    "gemini_text = []\n",
    "leo_text = []\n",
    "libra_text = []\n",
    "pisces_text = []\n",
    "sagittarus_text = []\n",
    "scorpio_text = []\n",
    "taurus_text = []\n",
    "virgo_text = []\n",
    "\n",
    "for i, item in enumerate(blog_df.sign):\n",
    "    if i >= 40000 and i <= 50000:       #<-- This is the part of the code doing the actual slice\n",
    "        if item == 'Aquarius':\n",
    "            aquarius_text.append(blog_df[blog_df.sign == 'Aquarius'].text[i]) #<-- This appends all the entries\n",
    "        elif item == 'Aries':                                                 #for aquarius star sign to the\n",
    "            aries_text.append(blog_df[blog_df.sign == 'Aries'].text[i])       #aquarius list above. The same is\n",
    "        elif item == 'Cancer':                                                #repeated for every star sign.\n",
    "            cancer_text.append(blog_df[blog_df.sign == 'Cancer'].text[i])\n",
    "        elif item == 'Capricorn':\n",
    "            capricorn_text.append(blog_df[blog_df.sign == 'Capricorn'].text[i])\n",
    "        elif item == 'Gemini':\n",
    "            gemini_text.append(blog_df[blog_df.sign == 'Gemini'].text[i])\n",
    "        elif item == 'Leo':\n",
    "            leo_text.append(blog_df[blog_df.sign == 'Leo'].text[i])\n",
    "        elif item == 'Libra':\n",
    "            libra_text.append(blog_df[blog_df.sign == 'Libra'].text[i])\n",
    "        elif item == 'Pisces':\n",
    "            pisces_text.append(blog_df[blog_df.sign == 'Pisces'].text[i])\n",
    "        elif item == 'Sagittarus':\n",
    "            sagittarus_text.append(blog_df[blog_df.sign == 'Sagittarus'].text[i])\n",
    "        elif item == 'Scorpio':\n",
    "            scorpio_text.append(blog_df[blog_df.sign == 'Scorpio'].text[i])\n",
    "        elif item == 'Taurus':\n",
    "            taurus_text.append(blog_df[blog_df.sign == 'Taurus'].text[i])\n",
    "        elif item == 'Virgo':\n",
    "            virgo_text.append(blog_df[blog_df.sign == 'Virgo'].text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd2f3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wrote the code in this cell myself. It casts all of the list from the cell above into strings and stores them\n",
    "#into variables of the same name. On second thoughts I should have called them something different. The code on this\n",
    "# cell is necessary but I cannot remember why.\n",
    "\n",
    "aquarius_text = str(aquarius_text)\n",
    "aries_text = str(aries_text)\n",
    "cancer_text = str(cancer_text)\n",
    "capricorn_text = str(capricorn_text)\n",
    "gemini_text = str(gemini_text)\n",
    "leo_text = str(leo_text)\n",
    "libra_text = str(libra_text)\n",
    "pisces_text = str(pisces_text)\n",
    "sagittarus_text = str(sagittarus_text)\n",
    "scorpio_text = str(scorpio_text)\n",
    "taurus_text = str(taurus_text)\n",
    "virgo_text = str(virgo_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9275286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wrote the code in this cell myself. This code concatenates all of the strings from the cell above into one long\n",
    "#string separated by three newlines so as to make it easier to split into chapters.\n",
    "\n",
    "sign_text = aquarius_text + \"\\n\\n\\n\" + aries_text + \"\\n\\n\\n\" + cancer_text\\\n",
    "            + \"\\n\\n\\n\" + capricorn_text + \"\\n\\n\\n\" + gemini_text\\\n",
    "            + \"\\n\\n\\n\" + leo_text + \"\\n\\n\\n\" + libra_text + \"\\n\\n\\n\" + pisces_text\\\n",
    "            + \"\\n\\n\\n\" + sagittarus_text + \"\\n\\n\\n\" + scorpio_text\\\n",
    "            + \"\\n\\n\\n\" + taurus_text + \"\\n\\n\\n\" + virgo_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63436cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wrote this code myself. This a regex that splits the long string from above into chapters and stores them into\n",
    "#a variable called signs_occupation. \n",
    "\n",
    "signs_occupation = re.split(r'\\n\\n\\n', sign_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60075c4b",
   "metadata": {},
   "source": [
    "# ZODIAC BOW TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d0baaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 71632)\n"
     ]
    }
   ],
   "source": [
    "#Using the CountVectorizer to get a bag of words using a custom tokeniser. This code was copied from lecture 2.2,\n",
    "#it tokenises all the words from each chapter into vectors BoW as explained in the critical report.\n",
    "\n",
    "count_vectoriser = CountVectorizer(tokenizer=my_tokeniser)\n",
    "bag_of_words = count_vectoriser.fit_transform(signs_occupation)\n",
    "print(bag_of_words.todense().shape) #<--This shows how many documents there are, and the number of words contained\n",
    "                                    #across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f187941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code was copied from lecture 2.2. I don't exaclty what it does but I believe it counts the TF from the BoW, as\n",
    "#explained in the critical report.\n",
    "\n",
    "vocab = count_vectoriser.get_feature_names_out()\n",
    "bag_of_words_df = pd.DataFrame(bag_of_words.todense(), columns = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e45974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state           474\n",
      "learning        479\n",
      "time            485\n",
      "president       504\n",
      "literacy        520\n",
      "la              590\n",
      "&               592\n",
      "&nbsp           669\n",
      "education       695\n",
      "ha              697\n",
      "library         804\n",
      "digest          843\n",
      "school          875\n",
      "eric            975\n",
      "information    1003\n",
      "student        1042\n",
      "'              1214\n",
      "urllink        1350\n",
      "wa             1374\n",
      "\"              1607\n",
      "Name: 0, dtype: int64 \n",
      " ha        159\n",
      "it's      174\n",
      "want      182\n",
      "day       185\n",
      "love      189\n",
      "think     202\n",
      "got       206\n",
      "thing     206\n",
      "really    212\n",
      "time      238\n",
      "don't     263\n",
      "know      276\n",
      "people    281\n",
      "'         308\n",
      "&nbsp     341\n",
      "like      392\n",
      "i'm       443\n",
      "just      465\n",
      "\"         580\n",
      "wa        671\n",
      "Name: 1, dtype: int64 \n",
      " ha          547\n",
      "people      550\n",
      "urllink     567\n",
      "really      620\n",
      "got         634\n",
      "thing       641\n",
      "don't       650\n",
      "day         671\n",
      "know        680\n",
      "good        697\n",
      "think       745\n",
      "'           906\n",
      "it's        908\n",
      "time       1127\n",
      "i'm        1181\n",
      "like       1206\n",
      "&nbsp      1246\n",
      "just       1249\n",
      "\"          1654\n",
      "wa         2634\n",
      "Name: 2, dtype: int64 \n",
      " people      538\n",
      "don't       564\n",
      "thing       568\n",
      "ha          578\n",
      "today       578\n",
      "going       592\n",
      "really      651\n",
      "think       666\n",
      "time        692\n",
      "know        695\n",
      "got         715\n",
      "good        717\n",
      "day         836\n",
      "urllink     876\n",
      "like        933\n",
      "just        980\n",
      "i'm        1022\n",
      "'          1770\n",
      "wa         2436\n",
      "\"          2514\n",
      "Name: 3, dtype: int64 \n",
      " ha         332\n",
      "people     332\n",
      "good       335\n",
      "think      379\n",
      "got        417\n",
      "don't      421\n",
      "thing      460\n",
      "really     465\n",
      "love       469\n",
      "know       506\n",
      "day        513\n",
      "it's       525\n",
      "'          547\n",
      "like       584\n",
      "time       626\n",
      "just       677\n",
      "i'm        783\n",
      "\"          898\n",
      "&nbsp     1223\n",
      "wa        1580\n",
      "Name: 4, dtype: int64 \n",
      " work        267\n",
      "got         285\n",
      "think       294\n",
      "really      333\n",
      "good        337\n",
      "day         356\n",
      "know        374\n",
      "ha          375\n",
      "people      381\n",
      "thing       389\n",
      "it's        394\n",
      "i'm         477\n",
      "time        480\n",
      "just        499\n",
      "like        509\n",
      "'           696\n",
      "urllink     963\n",
      "&nbsp      1047\n",
      "\"          1179\n",
      "wa         1240\n",
      "Name: 5, dtype: int64 \n",
      " went       348\n",
      "&nbsp      349\n",
      "really     355\n",
      "got        362\n",
      "ok         367\n",
      "know       374\n",
      "don't      375\n",
      "car        389\n",
      "time       422\n",
      "work       426\n",
      "i'll       440\n",
      "day        459\n",
      "like       492\n",
      "just       543\n",
      "today      567\n",
      "going      587\n",
      "it's       648\n",
      "wa        1225\n",
      "i'm       1408\n",
      "\"         1814\n",
      "Name: 6, dtype: int64 \n",
      " day        169\n",
      "lorx       173\n",
      "home       187\n",
      "&nbsp      192\n",
      "coz        212\n",
      "know       250\n",
      "got        250\n",
      "hahax      273\n",
      "time       283\n",
      "i'm        295\n",
      "just       308\n",
      "says:      331\n",
      "urllink    337\n",
      "like       350\n",
      "went       350\n",
      "u          354\n",
      "'          502\n",
      "den        569\n",
      "wa         779\n",
      "\"          790\n",
      "Name: 7, dtype: int64 \n",
      " delusional              0\n",
      "dek                     0\n",
      "deman                   0\n",
      "demand'                 0\n",
      "demeanor                0\n",
      "demeaning               0\n",
      "demasido                0\n",
      "demasiados              0\n",
      "demasiado               0\n",
      "demaning                0\n",
      "demand                  0\n",
      "ï¿½rachel                 0\n",
      "demanding               0\n",
      "demanded                0\n",
      "demande                 0\n",
      "demandas                0\n",
      "demanda                 0\n",
      "demand:                 0\n",
      "demanding/irritating    0\n",
      "[]                      1\n",
      "Name: 8, dtype: int64 \n",
      " work       149\n",
      "want       159\n",
      "thing      161\n",
      "really     163\n",
      "going      172\n",
      "think      182\n",
      "good       183\n",
      "ha         186\n",
      "day        191\n",
      "don't      193\n",
      "urllink    236\n",
      "know       251\n",
      "time       271\n",
      "just       288\n",
      "i'm        305\n",
      "like       311\n",
      "'          416\n",
      "&nbsp      455\n",
      "\"          549\n",
      "wa         705\n",
      "Name: 9, dtype: int64 \n",
      " good       317\n",
      "going      344\n",
      "i've       345\n",
      "ha         371\n",
      "got        386\n",
      "thing      410\n",
      "think      415\n",
      "don't      443\n",
      "day        454\n",
      "&nbsp      474\n",
      "really     513\n",
      "know       535\n",
      "time       590\n",
      "it's       591\n",
      "'          682\n",
      "just       748\n",
      "like       762\n",
      "i'm        769\n",
      "\"         1142\n",
      "wa        1611\n",
      "Name: 10, dtype: int64 \n",
      " going       286\n",
      "think       316\n",
      "ha          348\n",
      "don't       361\n",
      "it's        364\n",
      "thing       386\n",
      "day         387\n",
      "people      387\n",
      "got         402\n",
      "good        421\n",
      "know        430\n",
      "urllink     451\n",
      "really      468\n",
      "i'm         516\n",
      "time        555\n",
      "like        566\n",
      "'           679\n",
      "just        739\n",
      "\"          1148\n",
      "wa         1656\n",
      "Name: 11, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Topic 20 most common words (we've already removed STOP WORDS). This code was copied from lecture 2.2 although I\n",
    "#added some entries to it. It displays the 20 least common words from the blog entries according to star signs\n",
    "\n",
    "print(bag_of_words_df.iloc[0].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[1].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[2].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[3].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[4].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[5].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[6].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[7].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[8].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[9].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[10].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[11].sort_values()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66690761",
   "metadata": {},
   "source": [
    "Topic 0 = aquarius<br>\n",
    "Topic 1 = aries<br>\n",
    "Topic 2 = cancer<br>\n",
    "Topic 3 = capricorn<br>\n",
    "Topic 4 = gemini<br>\n",
    "Topic 5 = leo<br>\n",
    "Topic 6 = libra<br>\n",
    "Topic 7 = pisces<br>\n",
    "Topic 8 = sagittarus<br>\n",
    "Topic 9 = scorpio<br>\n",
    "Topic 10 = taurus<br>\n",
    "Topic 11 = virgo<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53f9016",
   "metadata": {},
   "source": [
    "# ZODIAC TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "103babc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 71632)\n"
     ]
    }
   ],
   "source": [
    "#Using the TFIDF Vectorizer to get TFIDF vectors with custom tokeniser. This code was copied from lecture 2.2\n",
    "#it tokenises all the words from each chapter into TF-IDF vectors as explained in the critical report\n",
    "\n",
    "tfidf_vectoriser = TfidfVectorizer(tokenizer=my_tokeniser)\n",
    "tfidf = tfidf_vectoriser.fit_transform(signs_occupation)\n",
    "print(tfidf.todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22d38a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signs occupation 0\n",
      "\"              0.278510\n",
      "digest         0.264529\n",
      "wa             0.238129\n",
      "urllink        0.233970\n",
      "eric           0.232415\n",
      "'              0.210399\n",
      "literacy       0.181793\n",
      "student        0.180590\n",
      "information    0.173831\n",
      "school         0.151647\n",
      "digest:        0.141013\n",
      "library        0.139342\n",
      "ha             0.120798\n",
      "education      0.120451\n",
      "&nbsp          0.115945\n",
      "librarian      0.113620\n",
      "los            0.108597\n",
      "&              0.102600\n",
      "la             0.102253\n",
      "president      0.087349\n",
      "Name: 0, dtype: float64\n",
      "Signs occupation 1\n",
      "wa        0.367346\n",
      "\"         0.317527\n",
      "just      0.254569\n",
      "i'm       0.242525\n",
      "like      0.214605\n",
      "&nbsp     0.186684\n",
      "spense    0.173226\n",
      "'         0.168618\n",
      "people    0.153836\n",
      "know      0.151099\n",
      "don't     0.143982\n",
      "time      0.130296\n",
      "really    0.116062\n",
      "got       0.112777\n",
      "thing     0.112777\n",
      "think     0.110587\n",
      "love      0.103470\n",
      "day       0.101280\n",
      "want      0.099638\n",
      "it's      0.095258\n",
      "Name: 1, dtype: float64\n",
      "Signs occupation 2\n",
      "wa       0.406212\n",
      "\"        0.255078\n",
      "maro     0.200931\n",
      "just     0.192619\n",
      "&nbsp    0.192157\n",
      "like     0.185988\n",
      "i'm      0.182132\n",
      "time     0.173805\n",
      "it's     0.140031\n",
      "'        0.139722\n",
      "dia      0.125094\n",
      "think    0.114893\n",
      "nak      0.109524\n",
      "good     0.107491\n",
      "saya     0.105796\n",
      "know     0.104869\n",
      "day      0.103481\n",
      "don't    0.100242\n",
      "thing    0.098854\n",
      "got      0.097775\n",
      "Name: 2, dtype: float64\n",
      "Signs occupation 3\n",
      "\"          0.434128\n",
      "wa         0.420658\n",
      "'          0.305651\n",
      "i'm        0.176483\n",
      "just       0.169230\n",
      "like       0.161114\n",
      "urllink    0.151271\n",
      "day        0.144364\n",
      "good       0.123814\n",
      "got        0.123469\n",
      "know       0.120015\n",
      "time       0.119497\n",
      "think      0.115008\n",
      "really     0.112417\n",
      "going      0.102229\n",
      "ha         0.099811\n",
      "today      0.099811\n",
      "thing      0.098085\n",
      "don't      0.097394\n",
      "people     0.092904\n",
      "Name: 3, dtype: float64\n",
      "Signs occupation 4\n",
      "wa               0.418467\n",
      "&nbsp            0.323914\n",
      "\"                0.237837\n",
      "i'm              0.207379\n",
      "freeatlasst:     0.189439\n",
      "just             0.179305\n",
      "time             0.165797\n",
      "like             0.154674\n",
      "'                0.144874\n",
      "it's             0.139047\n",
      "day              0.135869\n",
      "know             0.134015\n",
      "love             0.124216\n",
      "really           0.123156\n",
      "thing            0.121832\n",
      "don't            0.111503\n",
      "got              0.110443\n",
      "smahtcooky79:    0.108452\n",
      "think            0.100379\n",
      "good             0.088725\n",
      "Name: 4, dtype: float64\n",
      "Signs occupation 5\n",
      "wa         0.377971\n",
      "\"          0.359377\n",
      "&nbsp      0.319141\n",
      "urllink    0.293537\n",
      "'          0.212151\n",
      "like       0.155151\n",
      "just       0.152103\n",
      "time       0.146311\n",
      "i'm        0.145397\n",
      "it's       0.120097\n",
      "thing      0.118573\n",
      "people     0.116134\n",
      "ha         0.114306\n",
      "know       0.114001\n",
      "_____      0.111097\n",
      "day        0.108514\n",
      "good       0.102723\n",
      "really     0.101503\n",
      "think      0.089616\n",
      "got        0.086872\n",
      "Name: 5, dtype: float64\n",
      "Signs occupation 6\n",
      "\"         0.470294\n",
      "i'm       0.365035\n",
      "wa        0.317591\n",
      "it's      0.167999\n",
      "going     0.152184\n",
      "today     0.146999\n",
      "just      0.140777\n",
      "like      0.127555\n",
      "day       0.118999\n",
      "i'll      0.114074\n",
      "work      0.110444\n",
      "time      0.109407\n",
      "gti       0.101336\n",
      "car       0.100851\n",
      "don't     0.097222\n",
      "know      0.096963\n",
      "ok        0.095148\n",
      "got       0.093851\n",
      "really    0.092037\n",
      "&nbsp     0.090481\n",
      "Name: 6, dtype: float64\n",
      "Signs occupation 7\n",
      "\"          0.308944\n",
      "den        0.306055\n",
      "wa         0.304642\n",
      "hahax      0.215359\n",
      "'          0.196316\n",
      "says:      0.194043\n",
      "lorx       0.179892\n",
      "btden      0.147657\n",
      "sae        0.146456\n",
      "u          0.138438\n",
      "like       0.136874\n",
      "went       0.136874\n",
      "urllink    0.131790\n",
      "lahx       0.123741\n",
      "just       0.120449\n",
      "i'm        0.115365\n",
      "time       0.110672\n",
      "coz        0.104990\n",
      "hakx       0.098785\n",
      "got        0.097767\n",
      "Name: 7, dtype: float64\n",
      "Signs occupation 8\n",
      "[]              1.0\n",
      "\"               0.0\n",
      "obsered         0.0\n",
      "obscurring      0.0\n",
      "obscurity       0.0\n",
      "obscures        0.0\n",
      "obscured        0.0\n",
      "obscure         0.0\n",
      "obscuirty       0.0\n",
      "observacion     0.0\n",
      "obscenity       0.0\n",
      "obscenely       0.0\n",
      "obscene')       0.0\n",
      "obrien          0.0\n",
      "obra            0.0\n",
      "oboe            0.0\n",
      "observable      0.0\n",
      "observadores    0.0\n",
      "occurred        0.0\n",
      "observatory     0.0\n",
      "Name: 8, dtype: float64\n",
      "Signs occupation 9\n",
      "wa            0.416352\n",
      "\"             0.324223\n",
      "&nbsp         0.268709\n",
      "'             0.245677\n",
      "like          0.183667\n",
      "i'm           0.180124\n",
      "just          0.170084\n",
      "time          0.160044\n",
      "know          0.148233\n",
      "urllink       0.139374\n",
      "passiflora    0.124054\n",
      "don't         0.113980\n",
      "day           0.112799\n",
      "ha            0.109846\n",
      "good          0.108074\n",
      "think         0.107484\n",
      "going         0.101578\n",
      "pls           0.096497\n",
      "really        0.096263\n",
      "thing         0.095082\n",
      "Name: 9, dtype: float64\n",
      "Signs occupation 10\n",
      "wa        0.440529\n",
      "\"         0.312280\n",
      "i'm       0.210283\n",
      "like      0.208369\n",
      "just      0.204541\n",
      "'         0.186493\n",
      "it's      0.161609\n",
      "time      0.161336\n",
      "know      0.146296\n",
      "really    0.140280\n",
      "&nbsp     0.129616\n",
      "day       0.124147\n",
      "don't     0.121139\n",
      "think     0.113482\n",
      "thing     0.112115\n",
      "got       0.105552\n",
      "ha        0.101450\n",
      "i've      0.094340\n",
      "going     0.094067\n",
      "good      0.086684\n",
      "Name: 10, dtype: float64\n",
      "Signs occupation 11\n",
      "wa         0.491731\n",
      "\"          0.340886\n",
      "just       0.219438\n",
      "'          0.201622\n",
      "like       0.168068\n",
      "time       0.164801\n",
      "i'm        0.153221\n",
      "really     0.138968\n",
      "urllink    0.133920\n",
      "know       0.127684\n",
      "good       0.125011\n",
      "got        0.119370\n",
      "people     0.114915\n",
      "day        0.114915\n",
      "thing      0.114619\n",
      "it's       0.108086\n",
      "don't      0.107195\n",
      "ha         0.103335\n",
      "think      0.093833\n",
      "going      0.084925\n",
      "Name: 11, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#This code was copied from lecture 2.2. It displays the TF-IDF weight values for the least common vectors for\n",
    "#each document. according to star sign\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf.todense(), columns = vocab)\n",
    "for i in range(len(tfidf_df)):\n",
    "    print(\"Signs occupation\", i)\n",
    "    print(tfidf_df.iloc[i].sort_values(ascending = False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42a09ab",
   "metadata": {},
   "source": [
    "Topic 0 = aquarius<br>\n",
    "Topic 1 = aries<br>\n",
    "Topic 2 = cancer<br>\n",
    "Topic 3 = capricorn<br>\n",
    "Topic 4 = gemini<br>\n",
    "Topic 5 = leo<br>\n",
    "Topic 6 = libra<br>\n",
    "Topic 7 = pisces<br>\n",
    "Topic 8 = sagittarus<br>\n",
    "Topic 9 = scorpio<br>\n",
    "Topic 10 = taurus<br>\n",
    "Topic 11 = virgo<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2934eb33",
   "metadata": {},
   "source": [
    "# OCCUPATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3e93f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wrote the code in this cell myself. As explained in the critical report this takes a slice of the corpus\n",
    "#by occupation to analyse. It's a bit clunky but it works.\n",
    "\n",
    "student_text = []\n",
    "technology_text = []\n",
    "arts_text = []\n",
    "education_text = []\n",
    "communications_media_text = []\n",
    "internet_text = []\n",
    "non_profit_text = []\n",
    "engineering_text = []\n",
    "law_text = []\n",
    "publishing_text = []\n",
    "science_text = []\n",
    "government_text = []\n",
    "\n",
    "for i, item in enumerate(blog_df.topic):\n",
    "    if i >= 40000 and i <= 50000:        #<-- This is the part of the code doing the actual slice\n",
    "        if item == 'Student':\n",
    "            student_text.append(blog_df[blog_df.topic == item].text[i])   #<-- This appends all the entries\n",
    "        elif item == 'Technology':                                        #for student occupation to the list above\n",
    "            technology_text.append(blog_df[blog_df.topic == item].text[i]) #the same is repeated for every occupation\n",
    "        elif item == 'Arts':\n",
    "            arts_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Education':\n",
    "            education_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Communications-Media':\n",
    "            communications_media_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Internet':\n",
    "            internet_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Non-Profit':\n",
    "            non_profit_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Engineering':\n",
    "            engineering_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Law':\n",
    "            law_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Publishing':\n",
    "            publishing_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Science':\n",
    "            science_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Government':\n",
    "            government_text.append(blog_df[blog_df.topic == item].text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f8d1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wrote the code in this cell myself. It casts all of the list from the cell above into strings and stores them\n",
    "#into variables of the same name. On second thoughts I should have called them something different. The code on this\n",
    "# cell is necessary but I cannot remember why.\n",
    "\n",
    "student_text = str(student_text)\n",
    "technology_text = str(technology_text)\n",
    "arts_text = str(arts_text)\n",
    "education_text = str(education_text)\n",
    "communications_media_text = str(communications_media_text)\n",
    "internet_text = str(internet_text)\n",
    "non_profit_text = str(non_profit_text)\n",
    "engineering_text = str(engineering_text)\n",
    "law_text = str(law_text)\n",
    "publishing_text = str(publishing_text)\n",
    "science_text = str(science_text)\n",
    "government_text = str(government_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfa5f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wrote the code in this cell myself. This code concatenates all of the strings from the cell above into one long\n",
    "#string separated by three newlines so as to make it easier to split into chapters.\n",
    "\n",
    "topic_text = student_text + \"\\n\\n\\n\" + technology_text + \"\\n\\n\\n\" + arts_text + \"\\n\\n\\n\" + education_text\\\n",
    "            + \"\\n\\n\\n\" + communications_media_text + \"\\n\\n\\n\" + internet_text + \"\\n\\n\\n\" + non_profit_text\\\n",
    "            + \"\\n\\n\\n\" + engineering_text + \"\\n\\n\\n\" + law_text + \"\\n\\n\\n\" + publishing_text\\\n",
    "            + \"\\n\\n\\n\" + science_text + \"\\n\\n\\n\" + government_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8928b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wrote this code myself. This a regex that splits the long string from above into chapters and stores them into\n",
    "#a variable called signs_occupation.\n",
    "\n",
    "topics_occupation = re.split(r'\\n\\n\\n', topic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad2519",
   "metadata": {},
   "source": [
    "# OCCUPATIONS BOW TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cfd4b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 52484)\n"
     ]
    }
   ],
   "source": [
    "#Using the CountVectorizer to get a bag of words using a custom tokeniser. This code was copied from lecture 2.2,\n",
    "#it tokenises all the words from each chapter into vectors BoW as explained in the critical report.\n",
    "\n",
    "count_vectoriser = CountVectorizer(tokenizer=my_tokeniser)\n",
    "bag_of_words = count_vectoriser.fit_transform(topics_occupation)\n",
    "print(bag_of_words.todense().shape) #<--This shows how many documents there are, and the number of words contained\n",
    "                                    #across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4def016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code was copied from lecture 2.2. I don't exaclty what it does but I believe it counts the TF from the BoW, as\n",
    "#explained in the critical report.\n",
    "\n",
    "vocab = count_vectoriser.get_feature_names_out()\n",
    "bag_of_words_df = pd.DataFrame(bag_of_words.todense(), columns = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b27e3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going     1038\n",
      "thing     1052\n",
      "&nbsp     1097\n",
      "it's      1120\n",
      "people    1229\n",
      "think     1238\n",
      "good      1269\n",
      "went      1275\n",
      "time      1330\n",
      "day       1380\n",
      "know      1393\n",
      "don't     1424\n",
      "really    1470\n",
      "got       1513\n",
      "like      2045\n",
      "just      2148\n",
      "i'm       2357\n",
      "'         2656\n",
      "wa        4125\n",
      "\"         4442\n",
      "Name: 0, dtype: int64 \n",
      " really      53\n",
      "new         54\n",
      "want        56\n",
      "think       57\n",
      "good        61\n",
      "don't       64\n",
      "day         73\n",
      "know        74\n",
      "like        87\n",
      "ha          89\n",
      "hakx        95\n",
      "it's        95\n",
      "time       105\n",
      "just       106\n",
      "brought    106\n",
      "i'm        108\n",
      "wa         242\n",
      "'          284\n",
      "urllink    305\n",
      "\"          441\n",
      "Name: 1, dtype: int64 \n",
      " make        75\n",
      "really      76\n",
      "i've        76\n",
      "way         77\n",
      "it's        86\n",
      "know        88\n",
      "ha          93\n",
      "life        98\n",
      "game        99\n",
      "urllink    109\n",
      "thing      110\n",
      "like       113\n",
      "people     119\n",
      "just       120\n",
      "&nbsp      135\n",
      "i'm        137\n",
      "'          147\n",
      "time       163\n",
      "\"          223\n",
      "wa         270\n",
      "Name: 2, dtype: int64 \n",
      " new             420\n",
      "research        421\n",
      "state           476\n",
      "learning        487\n",
      "president       504\n",
      "literacy        521\n",
      "la              585\n",
      "ha              620\n",
      "&nbsp           678\n",
      "education       703\n",
      "library         802\n",
      "school          820\n",
      "digest          843\n",
      "eric            973\n",
      "information    1001\n",
      "'              1045\n",
      "student        1070\n",
      "wa             1212\n",
      "urllink        1352\n",
      "\"              1430\n",
      "Name: 3, dtype: int64 \n",
      " going       95\n",
      "don't       97\n",
      "think      101\n",
      "thing      114\n",
      "ha         117\n",
      "people     117\n",
      "u          117\n",
      "it's       122\n",
      "day        133\n",
      "know       138\n",
      "love       139\n",
      "i'm        145\n",
      "&nbsp      152\n",
      "just       159\n",
      "time       162\n",
      "like       192\n",
      "urllink    206\n",
      "'          324\n",
      "wa         336\n",
      "\"          414\n",
      "Name: 4, dtype: int64 \n",
      " today       44\n",
      "don't       44\n",
      "i'm         47\n",
      "life        49\n",
      "got         49\n",
      "ha          50\n",
      "thing       54\n",
      "work        56\n",
      "good        56\n",
      "think       57\n",
      "people      57\n",
      "know        61\n",
      "day         64\n",
      "just        69\n",
      "like        86\n",
      "time        87\n",
      "'           91\n",
      "urllink     92\n",
      "wa         168\n",
      "\"          196\n",
      "Name: 5, dtype: int64 \n",
      " did        14\n",
      "way        14\n",
      "getting    15\n",
      "going      15\n",
      "thing      17\n",
      "really     19\n",
      "think      20\n",
      "time       20\n",
      "oh         22\n",
      "ha         22\n",
      "it's       22\n",
      "day        25\n",
      "know       27\n",
      "god        28\n",
      "i'm        29\n",
      "&nbsp      29\n",
      "like       41\n",
      "\"          52\n",
      "wa         54\n",
      "just       56\n",
      "Name: 6, dtype: int64 \n",
      " photo         8\n",
      "director      8\n",
      "start         9\n",
      "need          9\n",
      "text          9\n",
      "try           9\n",
      "today        10\n",
      "character    10\n",
      "line         11\n",
      "player       12\n",
      "time         13\n",
      "page         13\n",
      "wa           14\n",
      "'            15\n",
      "3            17\n",
      "&nbsp        18\n",
      "button       20\n",
      "game         22\n",
      "urllink      32\n",
      "\"            44\n",
      "Name: 7, dtype: int64 \n",
      " don't      107\n",
      "going      108\n",
      "urllink    109\n",
      "new        111\n",
      "work       114\n",
      "got        115\n",
      "day        118\n",
      "ha         121\n",
      "i've       128\n",
      "think      132\n",
      "really     133\n",
      "know       135\n",
      "it's       137\n",
      "time       177\n",
      "like       197\n",
      "i'm        207\n",
      "just       260\n",
      "&nbsp      270\n",
      "\"          282\n",
      "wa         640\n",
      "Name: 8, dtype: int64 \n",
      " album         5\n",
      "new           5\n",
      "cd            5\n",
      "allied        5\n",
      "troop         6\n",
      "itâs          6\n",
      "i'm           6\n",
      "war           6\n",
      "currently     6\n",
      "gb            6\n",
      "going         7\n",
      "ha            7\n",
      "u             7\n",
      "player        8\n",
      "ipod          8\n",
      "music         8\n",
      "just          9\n",
      "like          9\n",
      "wa           13\n",
      "â            19\n",
      "Name: 9, dtype: int64 \n",
      " i'm        165\n",
      "u          165\n",
      "got        170\n",
      "people     175\n",
      "day        180\n",
      "know       181\n",
      "new        181\n",
      "look       183\n",
      "good       194\n",
      "urllink    202\n",
      "year       206\n",
      "film       222\n",
      "time       240\n",
      "it's       266\n",
      "just       271\n",
      "ha         297\n",
      "'          319\n",
      "like       370\n",
      "\"          558\n",
      "wa         673\n",
      "Name: 10, dtype: int64 \n",
      " don't      14\n",
      "away       14\n",
      "think      14\n",
      "work       15\n",
      "want       15\n",
      "new        15\n",
      "goliath    16\n",
      "say        17\n",
      "little     18\n",
      "love       18\n",
      "cream      19\n",
      "just       20\n",
      "law        20\n",
      "i'm        21\n",
      "like       23\n",
      "time       24\n",
      "ha         30\n",
      "'          32\n",
      "wa         53\n",
      "\"          84\n",
      "Name: 11, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Topic 20 most common words (we've already removed STOP WORDS). This code was copied from lecture 2.2 although I\n",
    "#added some entries to it. It displays the 20 least common words from the blog entries according to occupation\n",
    "\n",
    "print(  bag_of_words_df.iloc[0].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[1].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[2].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[3].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[4].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[5].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[6].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[7].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[8].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[9].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[10].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[11].sort_values()[-20:])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ff195",
   "metadata": {},
   "source": [
    "Topic 0 = student<br>\n",
    "Topic 1 = technology<br>\n",
    "Topic 2 = arts<br>\n",
    "Topic 3 = education<br>\n",
    "Topic 4 = communications_media<br>\n",
    "Topic 5 = internet<br>\n",
    "Topic 6 = non_profit<br>\n",
    "Topic 7 = engineering<br>\n",
    "Topic 8 = law<br>\n",
    "Topic 9 = publishing<br>\n",
    "Topic 10 = science<br>\n",
    "Topic 11 = government<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f25711d",
   "metadata": {},
   "source": [
    "# OCCUPATIONS TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "948bede7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 52484)\n"
     ]
    }
   ],
   "source": [
    "#Using the TFIDF Vectorizer to get TFIDF vectors with custom tokeniser. This code was copied from lecture 2.2\n",
    "#it tokenises all the words from each chapter into TF-IDF vectors as explained in the critical report\n",
    "\n",
    "tfidf_vectoriser = TfidfVectorizer(tokenizer=my_tokeniser)\n",
    "tfidf = tfidf_vectoriser.fit_transform(topics_occupation)#(chapters)\n",
    "print(tfidf.todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1561d183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topics occupation 0\n",
      "\"         0.390974\n",
      "wa        0.363073\n",
      "'         0.233775\n",
      "i'm       0.207458\n",
      "just      0.189062\n",
      "like      0.179996\n",
      "really    0.139742\n",
      "got       0.133171\n",
      "don't     0.125337\n",
      "know      0.122609\n",
      "day       0.121464\n",
      "den       0.119829\n",
      "time      0.117063\n",
      "went      0.112222\n",
      "good      0.111694\n",
      "think     0.108966\n",
      "people    0.108174\n",
      "&nbsp     0.104284\n",
      "it's      0.098580\n",
      "today     0.095538\n",
      "Name: 0, dtype: float64\n",
      "topics occupation 1\n",
      "\"                0.458087\n",
      "urllink          0.342176\n",
      "'                0.295004\n",
      "hakx             0.283392\n",
      "wa               0.251377\n",
      "ebot2004:        0.140204\n",
      "gremlinchief:    0.140204\n",
      "brought          0.118920\n",
      "i'm              0.112185\n",
      "just             0.110107\n",
      "time             0.109068\n",
      "it's             0.098681\n",
      "ha               0.092448\n",
      "like             0.090371\n",
      "cigar            0.083734\n",
      "node             0.076944\n",
      "know             0.076867\n",
      "day              0.075828\n",
      "ip               0.071733\n",
      "wireless         0.068150\n",
      "Name: 1, dtype: float64\n",
      "topics occupation 2\n",
      "wa         0.322434\n",
      "\"          0.266307\n",
      "time       0.194655\n",
      "'          0.175547\n",
      "&nbsp      0.174121\n",
      "i'm        0.163605\n",
      "just       0.143304\n",
      "people     0.142110\n",
      "urllink    0.140587\n",
      "like       0.134945\n",
      "thing      0.131362\n",
      "game       0.127689\n",
      "life       0.126399\n",
      "ha         0.111061\n",
      "know       0.105090\n",
      "it's       0.102701\n",
      "meme       0.098867\n",
      "i've       0.098024\n",
      "really     0.098024\n",
      "way        0.091953\n",
      "Name: 2, dtype: float64\n",
      "topics occupation 3\n",
      "eric           0.264855\n",
      "student        0.244005\n",
      "digest         0.229468\n",
      "urllink        0.224160\n",
      "\"              0.219521\n",
      "literacy       0.197256\n",
      "information    0.193981\n",
      "wa             0.186055\n",
      "'              0.160419\n",
      "library        0.155417\n",
      "education      0.147603\n",
      "los            0.139799\n",
      "school         0.135955\n",
      "digest:        0.134901\n",
      "la             0.122827\n",
      "&nbsp          0.112411\n",
      "president      0.105820\n",
      "learning       0.102251\n",
      "says:          0.099064\n",
      "librarian      0.097262\n",
      "Name: 3, dtype: float64\n",
      "topics occupation 4\n",
      "\"          0.366024\n",
      "wa         0.297063\n",
      "'          0.286454\n",
      "urllink    0.196706\n",
      "like       0.169750\n",
      "&nbsp      0.145142\n",
      "time       0.143227\n",
      "just       0.140574\n",
      "love       0.132729\n",
      "i'm        0.128197\n",
      "know       0.122008\n",
      "day        0.117587\n",
      "it's       0.107862\n",
      "people     0.103442\n",
      "ha         0.103442\n",
      "u          0.103442\n",
      "thing      0.100789\n",
      "think      0.089296\n",
      "don't      0.085759\n",
      "going      0.083991\n",
      "Name: 4, dtype: float64\n",
      "topics occupation 5\n",
      "\"          0.367544\n",
      "wa         0.315037\n",
      "skuse:     0.188484\n",
      "urllink    0.186329\n",
      "'          0.170645\n",
      "time       0.163144\n",
      "like       0.161269\n",
      "just       0.129390\n",
      "day        0.120014\n",
      "know       0.114389\n",
      "(me):      0.107705\n",
      "think      0.106888\n",
      "people     0.106888\n",
      "good       0.105012\n",
      "work       0.105012\n",
      "thing      0.101262\n",
      "life       0.099241\n",
      "ha         0.093761\n",
      "got        0.091886\n",
      "today      0.089114\n",
      "Name: 5, dtype: float64\n",
      "topics occupation 6\n",
      "just       0.286562\n",
      "wa         0.276327\n",
      "\"          0.266093\n",
      "like       0.209804\n",
      "god        0.167216\n",
      "&nbsp      0.160276\n",
      "i'm        0.148398\n",
      "know       0.138164\n",
      "oh         0.131384\n",
      "day        0.127929\n",
      "it's       0.112578\n",
      "ha         0.112578\n",
      "really     0.105009\n",
      "time       0.102343\n",
      "think      0.102343\n",
      "getting    0.089580\n",
      "adrian     0.088173\n",
      "thing      0.086992\n",
      "did        0.077375\n",
      "going      0.076758\n",
      "Name: 6, dtype: float64\n",
      "topics occupation 7\n",
      "\"              0.369102\n",
      "urllink        0.289924\n",
      "button         0.249229\n",
      "game           0.199323\n",
      "jayce          0.168634\n",
      "&nbsp          0.163082\n",
      "3              0.154022\n",
      "programming    0.131233\n",
      "page           0.127270\n",
      "'              0.125830\n",
      "wa             0.117441\n",
      "line           0.116485\n",
      "&quot          0.109656\n",
      "time           0.109053\n",
      "player         0.108722\n",
      "character      0.105896\n",
      "text           0.095306\n",
      "software       0.095071\n",
      "director       0.091787\n",
      "today          0.090601\n",
      "Name: 7, dtype: float64\n",
      "topics occupation 8\n",
      "wa         0.496577\n",
      "&nbsp      0.226262\n",
      "\"          0.218804\n",
      "just       0.201734\n",
      "i'm        0.160612\n",
      "gunner     0.153091\n",
      "like       0.152853\n",
      "time       0.137335\n",
      "really     0.111455\n",
      "i've       0.107265\n",
      "it's       0.106299\n",
      "know       0.104747\n",
      "think      0.102419\n",
      "ha         0.093884\n",
      "day        0.091556\n",
      "urllink    0.091343\n",
      "got        0.089229\n",
      "work       0.088453\n",
      "new        0.086125\n",
      "week       0.085119\n",
      "Name: 8, dtype: float64\n",
      "topics occupation 9\n",
      "â            0.306988\n",
      "ipod         0.181563\n",
      "gb           0.167310\n",
      "wa           0.166390\n",
      "napster      0.157836\n",
      "99]          0.147027\n",
      "allied       0.139425\n",
      "like         0.115193\n",
      "just         0.115193\n",
      "troop        0.114080\n",
      "player       0.110589\n",
      "music        0.110589\n",
      "jukebox      0.110270\n",
      "war          0.096943\n",
      "itâs         0.096943\n",
      "walkman      0.094701\n",
      "going        0.089594\n",
      "ha           0.089594\n",
      "u            0.089594\n",
      "currently    0.082942\n",
      "Name: 9, dtype: float64\n",
      "topics occupation 10\n",
      "wa            0.334977\n",
      "\"             0.277737\n",
      "like          0.184163\n",
      "'             0.158778\n",
      "ë§:            0.151517\n",
      "ëì            0.151517\n",
      "film          0.151130\n",
      "ha            0.147828\n",
      "just          0.134887\n",
      "it's          0.132398\n",
      "time          0.119457\n",
      "passiflora    0.112923\n",
      "urllink       0.108591\n",
      "year          0.102534\n",
      "good          0.096561\n",
      "look          0.091086\n",
      "new           0.090090\n",
      "know          0.090090\n",
      "day           0.089593\n",
      "people        0.087104\n",
      "Name: 10, dtype: float64\n",
      "topics occupation 11\n",
      "\"             0.416548\n",
      "wa            0.262822\n",
      "goliath       0.195685\n",
      "'             0.158685\n",
      "cream         0.152545\n",
      "ha            0.148767\n",
      "pav           0.128169\n",
      "time          0.119014\n",
      "law           0.115746\n",
      "like          0.114055\n",
      "i'm           0.104137\n",
      "just          0.099178\n",
      "little        0.096405\n",
      "love          0.096405\n",
      "mum           0.095764\n",
      "say           0.091049\n",
      "vertigo       0.086430\n",
      "christian'    0.085612\n",
      "goldie        0.085446\n",
      "cat           0.075119\n",
      "Name: 11, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#This code was copied from lecture 2.2. It displays the TF-IDF weight values for the least common vectors for\n",
    "#each document according to occupation.\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf.todense(), columns = vocab)\n",
    "for i in range(len(tfidf_df)):\n",
    "    print(\"topics occupation\", i)\n",
    "    print(tfidf_df.iloc[i].sort_values(ascending = False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820952ea",
   "metadata": {},
   "source": [
    "Topic 0 = student<br>\n",
    "Topic 1 = technology<br>\n",
    "Topic 2 = arts<br>\n",
    "Topic 3 = education<br>\n",
    "Topic 4 = communications_media<br>\n",
    "Topic 5 = internet<br>\n",
    "Topic 6 = non_profit<br>\n",
    "Topic 7 = engineering<br>\n",
    "Topic 8 = law<br>\n",
    "Topic 9 = publishing<br>\n",
    "Topic 10 = science<br>\n",
    "Topic 11 = government<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7aabee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
