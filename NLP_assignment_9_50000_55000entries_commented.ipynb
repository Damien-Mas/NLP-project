{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67f525a0",
   "metadata": {},
   "source": [
    "# MOST OF THIS CODE WAS COPIED FROM LECTURE 2.2. THE PIECES OF CODE WHICH HAVE NOT BEEN COPIED WILL BE POINTED OUT AS PART OF THE COMMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c37c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell imports all the correct libraries to run the code successfully\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d301bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(681284, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This cell loads the corpus which comes as a CSV file, into a pandas dataframe. It was copied from lecture 2.2\n",
    "\n",
    "blog_df = pd.read_csv('data/blogtext.csv', encoding='utf-8')\n",
    "blog_df.shape #<--This part shows how many words are contained in the entire document and how many columns the \n",
    "              #CSV file contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "323d9bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>I had an interesting conversation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Somehow Coca-Cola has a way of su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>If anything, Korea is a country o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Take a read of this news article ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>I surf the English news sites a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>Ah, the Korean language...it look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>If you click on my profile you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>Last night was pretty fun...mostl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>There is so much that is differen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>urlLink    Here it is, the super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>One thing I love about Seoul (and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>urlLink    Wonderful oh-gyup-sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>18,June,2004</td>\n",
       "      <td>Here is the latest from the Korea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>17,June,2004</td>\n",
       "      <td>Well, I stand corrected, again.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>16,June,2004</td>\n",
       "      <td>So I've been in Vancouver a few d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id gender  age              topic      sign          date  \\\n",
       "0   2059027   male   15            Student       Leo   14,May,2004   \n",
       "1   2059027   male   15            Student       Leo   13,May,2004   \n",
       "2   2059027   male   15            Student       Leo   12,May,2004   \n",
       "3   2059027   male   15            Student       Leo   12,May,2004   \n",
       "4   3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "5   3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "6   3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "7   3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "8   3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "9   3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "10  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "11  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "12  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "13  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "14  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "15  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "16  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "17  3581210   male   33  InvestmentBanking  Aquarius  18,June,2004   \n",
       "18  3581210   male   33  InvestmentBanking  Aquarius  17,June,2004   \n",
       "19  3581210   male   33  InvestmentBanking  Aquarius  16,June,2004   \n",
       "\n",
       "                                                 text  \n",
       "0              Info has been found (+/- 100 pages,...  \n",
       "1              These are the team members:   Drewe...  \n",
       "2              In het kader van kernfusie op aarde...  \n",
       "3                    testing!!!  testing!!!            \n",
       "4                Thanks to Yahoo!'s Toolbar I can ...  \n",
       "5                I had an interesting conversation...  \n",
       "6                Somehow Coca-Cola has a way of su...  \n",
       "7                If anything, Korea is a country o...  \n",
       "8                Take a read of this news article ...  \n",
       "9                I surf the English news sites a l...  \n",
       "10               Ah, the Korean language...it look...  \n",
       "11               If you click on my profile you'll...  \n",
       "12               Last night was pretty fun...mostl...  \n",
       "13               There is so much that is differen...  \n",
       "14                urlLink    Here it is, the super...  \n",
       "15               One thing I love about Seoul (and...  \n",
       "16                urlLink    Wonderful oh-gyup-sal...  \n",
       "17               Here is the latest from the Korea...  \n",
       "18               Well, I stand corrected, again.  ...  \n",
       "19               So I've been in Vancouver a few d...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This cell provides a sample of the first 20 entries in the pandas dataframe\n",
    "\n",
    "blog_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e149cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the tokeniser, as explained in the critical report. This code was copied from lecture 2.2.\n",
    "\n",
    "def my_tokeniser(doc):\n",
    "    #Split on spaces\n",
    "    tokens = re.split(r'[-\\s.,;!?]+', doc)\n",
    "    processed = []\n",
    "    for t in tokens:\n",
    "        #Lemmatise and make lowercase\n",
    "        t = lem.lemmatize(t.lower())\n",
    "        #Remove stop words\n",
    "        if not t in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            processed = processed + [t]\n",
    "    #Return an array of tokens for that document\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc6be9",
   "metadata": {},
   "source": [
    "# Zodiac "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92b6c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wrote the code in this cell myself. As explained in the critical report this takes a slice of the corpus\n",
    "#by star sign to analyse. It's a bit clunky but it works.\n",
    "\n",
    "aquarius_text = [] #<-- The entries for the aquarius star sign are loaded in this list. The same is repeated\n",
    "aries_text = []    #for every star sign.\n",
    "cancer_text = []\n",
    "capricorn_text = []\n",
    "gemini_text = []\n",
    "leo_text = []\n",
    "libra_text = []\n",
    "pisces_text = []\n",
    "sagittarus_text = []\n",
    "scorpio_text = []\n",
    "taurus_text = []\n",
    "virgo_text = []\n",
    "\n",
    "for i, item in enumerate(blog_df.sign):\n",
    "    if i >= 50000 and i <= 55000:       #<-- This is the part of the code doing the actual slice\n",
    "        if item == 'Aquarius':\n",
    "            aquarius_text.append(blog_df[blog_df.sign == 'Aquarius'].text[i]) #<-- This appends all the entries\n",
    "        elif item == 'Aries':                                                 #for aquarius star sign to the\n",
    "            aries_text.append(blog_df[blog_df.sign == 'Aries'].text[i])       #aquarius list above. The same is\n",
    "        elif item == 'Cancer':                                                #repeated for every star sign.\n",
    "            cancer_text.append(blog_df[blog_df.sign == 'Cancer'].text[i])\n",
    "        elif item == 'Capricorn':\n",
    "            capricorn_text.append(blog_df[blog_df.sign == 'Capricorn'].text[i])\n",
    "        elif item == 'Gemini':\n",
    "            gemini_text.append(blog_df[blog_df.sign == 'Gemini'].text[i])\n",
    "        elif item == 'Leo':\n",
    "            leo_text.append(blog_df[blog_df.sign == 'Leo'].text[i])\n",
    "        elif item == 'Libra':\n",
    "            libra_text.append(blog_df[blog_df.sign == 'Libra'].text[i])\n",
    "        elif item == 'Pisces':\n",
    "            pisces_text.append(blog_df[blog_df.sign == 'Pisces'].text[i])\n",
    "        elif item == 'Sagittarus':\n",
    "            sagittarus_text.append(blog_df[blog_df.sign == 'Sagittarus'].text[i])\n",
    "        elif item == 'Scorpio':\n",
    "            scorpio_text.append(blog_df[blog_df.sign == 'Scorpio'].text[i])\n",
    "        elif item == 'Taurus':\n",
    "            taurus_text.append(blog_df[blog_df.sign == 'Taurus'].text[i])\n",
    "        elif item == 'Virgo':\n",
    "            virgo_text.append(blog_df[blog_df.sign == 'Virgo'].text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29dff690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wrote the code in this cell myself. It casts all of the list from the cell above into strings and stores them\n",
    "#into variables of the same name. On second thoughts I should have called them something different. The code on this\n",
    "# cell is necessary but I cannot remember why.\n",
    "\n",
    "aquarius_text = str(aquarius_text)\n",
    "aries_text = str(aries_text)\n",
    "cancer_text = str(cancer_text)\n",
    "capricorn_text = str(capricorn_text)\n",
    "gemini_text = str(gemini_text)\n",
    "leo_text = str(leo_text)\n",
    "libra_text = str(libra_text)\n",
    "pisces_text = str(pisces_text)\n",
    "sagittarus_text = str(sagittarus_text)\n",
    "scorpio_text = str(scorpio_text)\n",
    "taurus_text = str(taurus_text)\n",
    "virgo_text = str(virgo_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e09281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wrote the code in this cell myself. This code concatenates all of the strings from the cell above into one long\n",
    "#string separated by three newlines so as to make it easier to split into chapters.\n",
    "\n",
    "sign_text = aquarius_text + \"\\n\\n\\n\" + aries_text + \"\\n\\n\\n\" + cancer_text\\\n",
    "            + \"\\n\\n\\n\" + capricorn_text + \"\\n\\n\\n\" + gemini_text\\\n",
    "            + \"\\n\\n\\n\" + leo_text + \"\\n\\n\\n\" + libra_text + \"\\n\\n\\n\" + pisces_text\\\n",
    "            + \"\\n\\n\\n\" + sagittarus_text + \"\\n\\n\\n\" + scorpio_text\\\n",
    "            + \"\\n\\n\\n\" + taurus_text + \"\\n\\n\\n\" + virgo_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5ee96d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wrote this code myself. This a regex that splits the long string from above into chapters and stores them into\n",
    "#a variable called signs_occupation. \n",
    "\n",
    "signs_occupation = re.split(r'\\n\\n\\n', sign_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b115c9fe",
   "metadata": {},
   "source": [
    "# ZODIAC BOW TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda6e582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 48972)\n"
     ]
    }
   ],
   "source": [
    "#Using the CountVectorizer to get a bag of words using a custom tokeniser. This code was copied from lecture 2.2,\n",
    "#it tokenises all the words from each chapter into vectors BoW as explained in the critical report.\n",
    "\n",
    "count_vectoriser = CountVectorizer(tokenizer=my_tokeniser)\n",
    "bag_of_words = count_vectoriser.fit_transform(signs_occupation)\n",
    "print(bag_of_words.todense().shape) #<--This shows how many documents there are, and the number of words contained\n",
    "                                    #across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1973694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code was copied from lecture 2.2. I don't exaclty what it does but I believe it counts the TF from the BoW, as\n",
    "#explained in the critical report.\n",
    "\n",
    "vocab = count_vectoriser.get_feature_names_out()\n",
    "bag_of_words_df = pd.DataFrame(bag_of_words.todense(), columns = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "206d0a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeah                117\n",
      "work                118\n",
      "really              120\n",
      "going               125\n",
      "think               129\n",
      "time                132\n",
      "don't               146\n",
      "know                149\n",
      "urllink             160\n",
      "love                177\n",
      "cherryvixxen840:    186\n",
      "it's                233\n",
      "'                   241\n",
      "like                244\n",
      "just                272\n",
      "pixxxie87:          279\n",
      "i'm                 351\n",
      "wa                  351\n",
      "\"                   444\n",
      "&nbsp               654\n",
      "Name: 0, dtype: int64 \n",
      " good       111\n",
      "ha         114\n",
      "need       116\n",
      "friend     131\n",
      "going      135\n",
      "think      139\n",
      "thing      140\n",
      "urllink    143\n",
      "really     151\n",
      "don't      157\n",
      "day        158\n",
      "i'm        187\n",
      "'          189\n",
      "know       191\n",
      "time       197\n",
      "just       262\n",
      "&nbsp      293\n",
      "like       295\n",
      "wa         431\n",
      "\"          470\n",
      "Name: 1, dtype: int64 \n",
      " ha         189\n",
      "it's       190\n",
      "good       192\n",
      "don't      193\n",
      "urllink    194\n",
      "want       196\n",
      "day        199\n",
      "going      226\n",
      "think      235\n",
      "&nbsp      256\n",
      "thing      260\n",
      "time       267\n",
      "really     276\n",
      "know       281\n",
      "like       334\n",
      "'          344\n",
      "i'm        357\n",
      "just       405\n",
      "wa         615\n",
      "\"          628\n",
      "Name: 2, dtype: int64 \n",
      " don't     141\n",
      "thing     149\n",
      "think     155\n",
      "got       158\n",
      "ha        162\n",
      "going     164\n",
      "day       172\n",
      "know      180\n",
      "really    187\n",
      "it's      202\n",
      "time      218\n",
      "&nbsp     224\n",
      "like      230\n",
      "i'm       245\n",
      "people    254\n",
      "just      267\n",
      ")         277\n",
      "'         368\n",
      "\"         439\n",
      "wa        598\n",
      "Name: 3, dtype: int64 \n",
      " ha          443\n",
      "really      456\n",
      "thing       469\n",
      "people      472\n",
      "don't       474\n",
      "good        484\n",
      "think       499\n",
      "day         527\n",
      "it's        549\n",
      "got         557\n",
      "&nbsp       567\n",
      "i'm         620\n",
      "time        679\n",
      "know        681\n",
      "'           739\n",
      "like        804\n",
      "urllink     949\n",
      "just       1090\n",
      "\"          1666\n",
      "wa         1711\n",
      "Name: 4, dtype: int64 \n",
      " people    220\n",
      "got       237\n",
      "ha        251\n",
      "going     261\n",
      "good      268\n",
      "think     274\n",
      "it's      277\n",
      "thing     279\n",
      "really    292\n",
      "don't     293\n",
      "day       299\n",
      "&nbsp     343\n",
      "know      363\n",
      "time      363\n",
      "'         459\n",
      "like      497\n",
      "just      525\n",
      "i'm       538\n",
      "\"         835\n",
      "wa        968\n",
      "Name: 5, dtype: int64 \n",
      " people      196\n",
      "went        200\n",
      "urllink     204\n",
      "ha          209\n",
      "work        211\n",
      "i'm         227\n",
      "thing       248\n",
      "good        252\n",
      "got         265\n",
      "think       270\n",
      "u           286\n",
      "day         297\n",
      "know        303\n",
      "time        348\n",
      "just        373\n",
      "&nbsp       412\n",
      "like        419\n",
      "'           684\n",
      "\"           786\n",
      "wa         1138\n",
      "Name: 6, dtype: int64 \n",
      " think      187\n",
      "going      192\n",
      "urllink    197\n",
      "really     203\n",
      "thing      214\n",
      "*          215\n",
      "good       216\n",
      "&nbsp      222\n",
      "know       231\n",
      "it's       243\n",
      "grace      248\n",
      "day        278\n",
      "time       295\n",
      "'          323\n",
      "just       347\n",
      "levi       356\n",
      "like       445\n",
      "”          485\n",
      "\"          510\n",
      "wa         901\n",
      "Name: 7, dtype: int64 \n",
      " deutsche       0\n",
      "deux           0\n",
      "dev            0\n",
      "dev's          0\n",
      "devalue        0\n",
      "devastated     0\n",
      "develops       0\n",
      "devastating    0\n",
      "develop        0\n",
      "develope       0\n",
      "developed      0\n",
      "developed)     0\n",
      "developer      0\n",
      "developer'     0\n",
      "developer:     0\n",
      "developing     0\n",
      "development    0\n",
      "devastation    0\n",
      "�that�s        0\n",
      "[]             1\n",
      "Name: 8, dtype: int64 \n",
      " going         105\n",
      "make          107\n",
      "experience    114\n",
      "e             119\n",
      "good          131\n",
      "time          133\n",
      "people        135\n",
      "don't         152\n",
      "thing         159\n",
      "life          167\n",
      "really        168\n",
      "know          177\n",
      "it's          188\n",
      "'             189\n",
      "like          193\n",
      "think         212\n",
      "i'm           254\n",
      "just          287\n",
      "wa            401\n",
      "\"             454\n",
      "Name: 9, dtype: int64 \n",
      " people      394\n",
      "thing       423\n",
      "love        442\n",
      "'           463\n",
      "want        520\n",
      "think       525\n",
      "fucking     529\n",
      "today       548\n",
      "time        569\n",
      "going       600\n",
      "i'm         640\n",
      "day         644\n",
      "really      678\n",
      "don't       717\n",
      "it's        886\n",
      "like        992\n",
      "\"          1212\n",
      "know       1266\n",
      "just       1362\n",
      "wa         2030\n",
      "Name: 10, dtype: int64 \n",
      " really     73\n",
      "make       75\n",
      "think      80\n",
      "thing      89\n",
      "life       89\n",
      "day        91\n",
      "work       91\n",
      "don't      92\n",
      "know       95\n",
      "ha        108\n",
      "i'm       121\n",
      "it's      140\n",
      "like      143\n",
      "time      148\n",
      "'         156\n",
      "just      185\n",
      "\"         206\n",
      "people    214\n",
      "&nbsp     295\n",
      "wa        310\n",
      "Name: 11, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Topic 20 most common words (we've already removed STOP WORDS). This code was copied from lecture 2.2 although I\n",
    "#added some entries to it. It displays the 20 least common words from the blog entries according to star signs\n",
    "\n",
    "print(bag_of_words_df.iloc[0].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[1].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[2].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[3].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[4].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[5].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[6].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[7].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[8].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[9].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[10].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[11].sort_values()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f22d40",
   "metadata": {},
   "source": [
    "Topic 0 = aquarius<br>\n",
    "Topic 1 = aries<br>\n",
    "Topic 2 = cancer<br>\n",
    "Topic 3 = capricorn<br>\n",
    "Topic 4 = gemini<br>\n",
    "Topic 5 = leo<br>\n",
    "Topic 6 = libra<br>\n",
    "Topic 7 = pisces<br>\n",
    "Topic 8 = sagittarus<br>\n",
    "Topic 9 = scorpio<br>\n",
    "Topic 10 = taurus<br>\n",
    "Topic 11 = virgo<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0b561",
   "metadata": {},
   "source": [
    "# ZODIAC TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dc81202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 48972)\n"
     ]
    }
   ],
   "source": [
    "#Using the TFIDF Vectorizer to get TFIDF vectors with custom tokeniser. This code was copied from lecture 2.2\n",
    "#it tokenises all the words from each chapter into TF-IDF vectors as explained in the critical report\n",
    "\n",
    "tfidf_vectoriser = TfidfVectorizer(tokenizer=my_tokeniser)\n",
    "tfidf = tfidf_vectoriser.fit_transform(signs_occupation)\n",
    "print(tfidf.todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f27272d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signs occupation 0\n",
      "pixxxie87:           0.457052\n",
      "&nbsp                0.402926\n",
      "cherryvixxen840:     0.304701\n",
      "\"                    0.273546\n",
      "i'm                  0.216249\n",
      "wa                   0.216249\n",
      "daintydewdrop488:    0.188390\n",
      "just                 0.167578\n",
      "like                 0.150327\n",
      "'                    0.148479\n",
      "it's                 0.143550\n",
      "love                 0.109049\n",
      "urllink              0.098575\n",
      "know                 0.091798\n",
      "don't                0.089950\n",
      "time                 0.081324\n",
      "think                0.079476\n",
      "going                0.077012\n",
      "really               0.073931\n",
      "work                 0.072699\n",
      "Name: 0, dtype: float64\n",
      "Signs occupation 1\n",
      "\"          0.368110\n",
      "wa         0.337565\n",
      "like       0.231048\n",
      "&nbsp      0.229481\n",
      "just       0.205202\n",
      "robyn:     0.156191\n",
      "time       0.154293\n",
      "know       0.149594\n",
      "'          0.148027\n",
      "i'm        0.146461\n",
      "happi      0.124952\n",
      "day        0.123748\n",
      "don't      0.122964\n",
      "really     0.118265\n",
      "urllink    0.111999\n",
      "thing      0.109650\n",
      "think      0.108867\n",
      "going      0.105734\n",
      "friend     0.102601\n",
      "(one)      0.102044\n",
      "Name: 1, dtype: float64\n",
      "Signs occupation 2\n",
      "\"          0.355797\n",
      "wa         0.348432\n",
      "just       0.229455\n",
      "i'm        0.202260\n",
      "'          0.194895\n",
      "like       0.189229\n",
      "know       0.159202\n",
      "really     0.156369\n",
      "time       0.151270\n",
      "thing      0.147304\n",
      "&nbsp      0.145038\n",
      "think      0.133140\n",
      "going      0.128041\n",
      "day        0.112745\n",
      "want       0.111045\n",
      "urllink    0.109912\n",
      "don't      0.109345\n",
      "good       0.108779\n",
      "it's       0.107646\n",
      "ha         0.107079\n",
      "Name: 2, dtype: float64\n",
      "Signs occupation 3\n",
      "wa        0.389654\n",
      "\"         0.286050\n",
      "'         0.239787\n",
      ")         0.180492\n",
      "just      0.173976\n",
      "people    0.165505\n",
      "i'm       0.159641\n",
      "like      0.149867\n",
      "&nbsp     0.145957\n",
      "time      0.142048\n",
      "it's      0.131622\n",
      "really    0.121848\n",
      "know      0.117287\n",
      "day       0.112074\n",
      "going     0.106862\n",
      "ha        0.105558\n",
      "got       0.102952\n",
      "think     0.100997\n",
      "thing     0.097088\n",
      "don't     0.091875\n",
      "Name: 3, dtype: float64\n",
      "Signs occupation 4\n",
      "wa         0.379448\n",
      "\"          0.369468\n",
      "just       0.241729\n",
      "urllink    0.210459\n",
      "like       0.178303\n",
      "'          0.163888\n",
      "know       0.151025\n",
      "time       0.150581\n",
      "i'm        0.137497\n",
      "[          0.130900\n",
      "&nbsp      0.125743\n",
      "got        0.123526\n",
      "it's       0.121751\n",
      "day        0.116872\n",
      "think      0.110663\n",
      "good       0.107336\n",
      "don't      0.105119\n",
      "people     0.104675\n",
      "thing      0.104010\n",
      "really     0.101127\n",
      "Name: 4, dtype: float64\n",
      "Signs occupation 5\n",
      "wa        0.399959\n",
      "\"         0.345006\n",
      "i'm       0.222291\n",
      "just      0.216920\n",
      "like      0.205351\n",
      "'         0.189650\n",
      "know      0.149985\n",
      "time      0.149985\n",
      "&nbsp     0.141721\n",
      "day       0.123541\n",
      "don't     0.121062\n",
      "really    0.120649\n",
      "thing     0.115277\n",
      "it's      0.114451\n",
      "think     0.113212\n",
      "good      0.110732\n",
      "going     0.107840\n",
      "ha        0.103708\n",
      "got       0.097924\n",
      "people    0.090900\n",
      "Name: 5, dtype: float64\n",
      "Signs occupation 6\n",
      "wa         0.468263\n",
      "\"          0.323422\n",
      "'          0.281451\n",
      "ladee      0.190375\n",
      "like       0.172409\n",
      "&nbsp      0.169529\n",
      "just       0.153481\n",
      "time       0.143195\n",
      "know       0.124678\n",
      "day        0.122209\n",
      "u          0.117683\n",
      "think      0.111099\n",
      "got        0.109042\n",
      "fh         0.106179\n",
      "good       0.103693\n",
      "thing      0.102047\n",
      "i'm        0.093406\n",
      "work       0.086822\n",
      "ha         0.085999\n",
      "urllink    0.083942\n",
      "Name: 6, dtype: float64\n",
      "Signs occupation 7\n",
      "wa         0.424153\n",
      "levi       0.275145\n",
      "”          0.266860\n",
      "\"          0.240087\n",
      "like       0.209487\n",
      "just       0.163353\n",
      "'          0.152055\n",
      "time       0.138874\n",
      "day        0.130871\n",
      "grace      0.126154\n",
      "it's       0.114394\n",
      "*          0.109367\n",
      "know       0.108745\n",
      "&nbsp      0.104508\n",
      "good       0.101684\n",
      "thing      0.100742\n",
      "really     0.095564\n",
      "urllink    0.092739\n",
      "going      0.090386\n",
      "think      0.088032\n",
      "Name: 7, dtype: float64\n",
      "Signs occupation 8\n",
      "[]                                                          1.0\n",
      "\"                                                           0.0\n",
      "org/default1                                                0.0\n",
      "oregan                                                      0.0\n",
      "oregano                                                     0.0\n",
      "oregon                                                      0.0\n",
      "oregon'                                                     0.0\n",
      "orentaion                                                   0.0\n",
      "oreo                                                        0.0\n",
      "org                                                         0.0\n",
      "org/                                                        0.0\n",
      "org/default                                                 0.0\n",
      "org/media/video/beautifulmidnight/apparitions_480           0.0\n",
      "oredi                                                       0.0\n",
      "org/media/video/whitelightrockandroll/alertstatusred_480    0.0\n",
      "org/rh/                                                     0.0\n",
      "org/victoria/photos/fall03/                                 0.0\n",
      "org/wnl/index                                               0.0\n",
      "organ                                                       0.0\n",
      "organic                                                     0.0\n",
      "Name: 8, dtype: float64\n",
      "Signs occupation 9\n",
      "\"             0.372529\n",
      "wa            0.329040\n",
      "just          0.235498\n",
      "i'm           0.208419\n",
      "think         0.173956\n",
      "like          0.158366\n",
      "'             0.155084\n",
      "it's          0.154263\n",
      "know          0.145237\n",
      "really        0.137852\n",
      "life          0.137032\n",
      "thing         0.130467\n",
      "don't         0.124723\n",
      "people        0.110774\n",
      "time          0.109133\n",
      "good          0.107492\n",
      "e             0.097645\n",
      "experience    0.093543\n",
      "make          0.087799\n",
      "going         0.086158\n",
      "Name: 9, dtype: float64\n",
      "Signs occupation 10\n",
      "wa         0.437224\n",
      "just       0.293350\n",
      "know       0.272673\n",
      "\"          0.261042\n",
      "like       0.213658\n",
      "it's       0.190828\n",
      "don't      0.154429\n",
      "really     0.146029\n",
      "day        0.138706\n",
      "i'm        0.137844\n",
      "fucking    0.133170\n",
      "going      0.129229\n",
      "time       0.122552\n",
      "today      0.118029\n",
      "think      0.113075\n",
      "want       0.111998\n",
      "cb         0.100334\n",
      "'          0.099722\n",
      "love       0.095199\n",
      "thing      0.091106\n",
      "Name: 10, dtype: float64\n",
      "Signs occupation 11\n",
      "wa        0.353804\n",
      "&nbsp     0.336684\n",
      "people    0.244239\n",
      "\"         0.235108\n",
      "just      0.211141\n",
      "'         0.178043\n",
      "time      0.168913\n",
      "like      0.163206\n",
      "it's      0.159782\n",
      "kimani    0.148700\n",
      "i'm       0.138098\n",
      "ha        0.123261\n",
      "gk        0.121388\n",
      "know      0.108424\n",
      "don't     0.105000\n",
      "work      0.103859\n",
      "day       0.103859\n",
      "thing     0.101576\n",
      "life      0.101576\n",
      "says:     0.094098\n",
      "Name: 11, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#This code was copied from lecture 2.2. It displays the TF-IDF weight values for the least common vectors for\n",
    "#each document. according to star sign\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf.todense(), columns = vocab)\n",
    "for i in range(len(tfidf_df)):\n",
    "    print(\"Signs occupation\", i)\n",
    "    print(tfidf_df.iloc[i].sort_values(ascending = False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ed43a",
   "metadata": {},
   "source": [
    "Topic 0 = aquarius<br>\n",
    "Topic 1 = aries<br>\n",
    "Topic 2 = cancer<br>\n",
    "Topic 3 = capricorn<br>\n",
    "Topic 4 = gemini<br>\n",
    "Topic 5 = leo<br>\n",
    "Topic 6 = libra<br>\n",
    "Topic 7 = pisces<br>\n",
    "Topic 8 = sagittarus<br>\n",
    "Topic 9 = scorpio<br>\n",
    "Topic 10 = taurus<br>\n",
    "Topic 11 = virgo<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cda809",
   "metadata": {},
   "source": [
    "# OCCUPATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e38406cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wrote the code in this cell myself. As explained in the critical report this takes a slice of the corpus\n",
    "#by occupation to analyse. It's a bit clunky but it works.\n",
    "\n",
    "student_text = []\n",
    "technology_text = []\n",
    "arts_text = []\n",
    "education_text = []\n",
    "communications_media_text = []\n",
    "internet_text = []\n",
    "non_profit_text = []\n",
    "engineering_text = []\n",
    "law_text = []\n",
    "publishing_text = []\n",
    "science_text = []\n",
    "government_text = []\n",
    "\n",
    "for i, item in enumerate(blog_df.topic):\n",
    "    if i >= 35000 and i <= 36000:        #<-- This is the part of the code doing the actual slice\n",
    "        if item == 'Student':\n",
    "            student_text.append(blog_df[blog_df.topic == item].text[i])   #<-- This appends all the entries\n",
    "        elif item == 'Technology':                                        #for student occupation to the list above\n",
    "            technology_text.append(blog_df[blog_df.topic == item].text[i]) #the same is repeated for every occupation\n",
    "        elif item == 'Arts':\n",
    "            arts_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Education':\n",
    "            education_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Communications-Media':\n",
    "            communications_media_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Internet':\n",
    "            internet_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Non-Profit':\n",
    "            non_profit_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Engineering':\n",
    "            engineering_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Law':\n",
    "            law_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Publishing':\n",
    "            publishing_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Science':\n",
    "            science_text.append(blog_df[blog_df.topic == item].text[i])\n",
    "        elif item == 'Government':\n",
    "            government_text.append(blog_df[blog_df.topic == item].text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a94f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wrote the code in this cell myself. It casts all of the list from the cell above into strings and stores them\n",
    "#into variables of the same name. On second thoughts I should have called them something different. The code on this\n",
    "# cell is necessary but I cannot remember why.\n",
    "\n",
    "student_text = str(student_text)\n",
    "technology_text = str(technology_text)\n",
    "arts_text = str(arts_text)\n",
    "education_text = str(education_text)\n",
    "communications_media_text = str(communications_media_text)\n",
    "internet_text = str(internet_text)\n",
    "non_profit_text = str(non_profit_text)\n",
    "engineering_text = str(engineering_text)\n",
    "law_text = str(law_text)\n",
    "publishing_text = str(publishing_text)\n",
    "science_text = str(science_text)\n",
    "government_text = str(government_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e034630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wrote the code in this cell myself. This code concatenates all of the strings from the cell above into one long\n",
    "#string separated by three newlines so as to make it easier to split into chapters.\n",
    "\n",
    "topic_text = student_text + \"\\n\\n\\n\" + technology_text + \"\\n\\n\\n\" + arts_text + \"\\n\\n\\n\" + education_text\\\n",
    "            + \"\\n\\n\\n\" + communications_media_text + \"\\n\\n\\n\" + internet_text + \"\\n\\n\\n\" + non_profit_text\\\n",
    "            + \"\\n\\n\\n\" + engineering_text + \"\\n\\n\\n\" + law_text + \"\\n\\n\\n\" + publishing_text\\\n",
    "            + \"\\n\\n\\n\" + science_text + \"\\n\\n\\n\" + government_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "371739b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wrote this code myself. This a regex that splits the long string from above into chapters and stores them into\n",
    "#a variable called signs_occupation.\n",
    "\n",
    "topics_occupation = re.split(r'\\n\\n\\n', topic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e480d99",
   "metadata": {},
   "source": [
    "# OCCUPATIONS BOW TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72009dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 12350)\n"
     ]
    }
   ],
   "source": [
    "#Using the CountVectorizer to get a bag of words using a custom tokeniser. This code was copied from lecture 2.2,\n",
    "#it tokenises all the words from each chapter into vectors BoW as explained in the critical report.\n",
    "\n",
    "count_vectoriser = CountVectorizer(tokenizer=my_tokeniser)\n",
    "bag_of_words = count_vectoriser.fit_transform(topics_occupation)\n",
    "print(bag_of_words.todense().shape) #<--This shows how many documents there are, and the number of words contained\n",
    "                                    #across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "472ab945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code was copied from lecture 2.2. I don't exaclty what it does but I believe it counts the TF from the BoW, as\n",
    "#explained in the critical report.\n",
    "\n",
    "vocab = count_vectoriser.get_feature_names_out()\n",
    "bag_of_words_df = pd.DataFrame(bag_of_words.todense(), columns = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "223258fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people      136\n",
      "captain     141\n",
      "going       142\n",
      "time        143\n",
      "thing       146\n",
      "it's        155\n",
      "kuronue:    159\n",
      "don't       175\n",
      "think       187\n",
      "i'm         200\n",
      "really      202\n",
      "jack        206\n",
      "baka:       209\n",
      "know        234\n",
      "just        241\n",
      "&nbsp       251\n",
      "like        259\n",
      "wa          433\n",
      "'           443\n",
      "\"           648\n",
      "Name: 0, dtype: int64 \n",
      " people      24\n",
      "u           24\n",
      "really      24\n",
      "year        24\n",
      "think       25\n",
      "way         26\n",
      "good        28\n",
      "day         29\n",
      "urllink     35\n",
      "credit      36\n",
      "ha          37\n",
      "know        38\n",
      "don't       38\n",
      "'           42\n",
      "just        42\n",
      "time        55\n",
      "like        56\n",
      "&nbsp       65\n",
      "wa         120\n",
      "\"          123\n",
      "Name: 1, dtype: int64 \n",
      " new         35\n",
      "going       36\n",
      "know        38\n",
      "i'm         39\n",
      ")           40\n",
      "good        40\n",
      "film        40\n",
      "&rarr       40\n",
      "night       41\n",
      "think       41\n",
      "video       43\n",
      "time        52\n",
      "like        59\n",
      "it's        60\n",
      "work        63\n",
      "just        65\n",
      "'           76\n",
      "wa         103\n",
      "urllink    136\n",
      "\"          220\n",
      "Name: 2, dtype: int64 \n",
      " life      23\n",
      "say       25\n",
      "ha        26\n",
      "i’m       28\n",
      "told      29\n",
      "today     30\n",
      "feel      32\n",
      "got       33\n",
      "'         33\n",
      "thing     33\n",
      "want      34\n",
      "really    35\n",
      "&nbsp     39\n",
      "think     40\n",
      "\"         45\n",
      "know      51\n",
      "just      58\n",
      "time      58\n",
      "like      58\n",
      "wa        90\n",
      "Name: 3, dtype: int64 \n",
      " stuff      20\n",
      "life       21\n",
      "people     21\n",
      "2004       22\n",
      "talk       22\n",
      "u          22\n",
      "ok         22\n",
      "work       24\n",
      "betting    24\n",
      "\"          26\n",
      "film       27\n",
      "scene      29\n",
      "idea       29\n",
      "think      31\n",
      "just       33\n",
      "bet        37\n",
      "story      37\n",
      "guy        42\n",
      "soccer     49\n",
      "like       59\n",
      "Name: 4, dtype: int64 \n",
      " day        10\n",
      "like       10\n",
      "way        10\n",
      "time       11\n",
      "i've       12\n",
      "urllink    12\n",
      "got        13\n",
      "'          13\n",
      "going      14\n",
      "went       14\n",
      "weekend    16\n",
      "jay        16\n",
      "know       16\n",
      "think      18\n",
      "just       20\n",
      "good       20\n",
      "i'm        28\n",
      "i'll       28\n",
      "wa         34\n",
      "\"          54\n",
      "Name: 5, dtype: int64 \n",
      " think       20\n",
      "officer     20\n",
      "film        21\n",
      "did         22\n",
      "just        23\n",
      "like        24\n",
      "time        25\n",
      "jp          26\n",
      "'i          26\n",
      "jr          27\n",
      "ha          27\n",
      "hussam      28\n",
      "said        35\n",
      "people      37\n",
      "u           39\n",
      "urllink     57\n",
      "\"           68\n",
      ":           89\n",
      "wa          96\n",
      "'          126\n",
      "Name: 6, dtype: int64 \n",
      " dissolve         0\n",
      "distance         0\n",
      "distance*        0\n",
      "distant          0\n",
      "disterbing       0\n",
      "distinct         0\n",
      "dodging          0\n",
      "distinction      0\n",
      "distinguished    0\n",
      "distract         0\n",
      "distracted       0\n",
      "distracting      0\n",
      "distraction      0\n",
      "district         0\n",
      "disturb          0\n",
      "disturbing       0\n",
      "ditch            0\n",
      "distinctive      0\n",
      "…                0\n",
      "[]               1\n",
      "Name: 7, dtype: int64 \n",
      " dissolve         0\n",
      "distance         0\n",
      "distance*        0\n",
      "distant          0\n",
      "disterbing       0\n",
      "distinct         0\n",
      "dodging          0\n",
      "distinction      0\n",
      "distinguished    0\n",
      "distract         0\n",
      "distracted       0\n",
      "distracting      0\n",
      "distraction      0\n",
      "district         0\n",
      "disturb          0\n",
      "disturbing       0\n",
      "ditch            0\n",
      "distinctive      0\n",
      "…                0\n",
      "[]               1\n",
      "Name: 8, dtype: int64 \n",
      " dissolve         0\n",
      "distance         0\n",
      "distance*        0\n",
      "distant          0\n",
      "disterbing       0\n",
      "distinct         0\n",
      "dodging          0\n",
      "distinction      0\n",
      "distinguished    0\n",
      "distract         0\n",
      "distracted       0\n",
      "distracting      0\n",
      "distraction      0\n",
      "district         0\n",
      "disturb          0\n",
      "disturbing       0\n",
      "ditch            0\n",
      "distinctive      0\n",
      "…                0\n",
      "[]               1\n",
      "Name: 9, dtype: int64 \n",
      " dissolve         0\n",
      "distance         0\n",
      "distance*        0\n",
      "distant          0\n",
      "disterbing       0\n",
      "distinct         0\n",
      "dodging          0\n",
      "distinction      0\n",
      "distinguished    0\n",
      "distract         0\n",
      "distracted       0\n",
      "distracting      0\n",
      "distraction      0\n",
      "district         0\n",
      "disturb          0\n",
      "disturbing       0\n",
      "ditch            0\n",
      "distinctive      0\n",
      "…                0\n",
      "[]               1\n",
      "Name: 10, dtype: int64 \n",
      " dissolve         0\n",
      "distance         0\n",
      "distance*        0\n",
      "distant          0\n",
      "disterbing       0\n",
      "distinct         0\n",
      "dodging          0\n",
      "distinction      0\n",
      "distinguished    0\n",
      "distract         0\n",
      "distracted       0\n",
      "distracting      0\n",
      "distraction      0\n",
      "district         0\n",
      "disturb          0\n",
      "disturbing       0\n",
      "ditch            0\n",
      "distinctive      0\n",
      "…                0\n",
      "[]               1\n",
      "Name: 11, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Topic 20 most common words (we've already removed STOP WORDS). This code was copied from lecture 2.2 although I\n",
    "#added some entries to it. It displays the 20 least common words from the blog entries according to occupation\n",
    "\n",
    "print(  bag_of_words_df.iloc[0].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[1].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[2].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[3].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[4].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[5].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[6].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[7].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[8].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[9].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[10].sort_values()[-20:],'\\n',\\\n",
    "        bag_of_words_df.iloc[11].sort_values()[-20:])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd87b2",
   "metadata": {},
   "source": [
    "Topic 0 = student<br>\n",
    "Topic 1 = technology<br>\n",
    "Topic 2 = arts<br>\n",
    "Topic 3 = education<br>\n",
    "Topic 4 = communications_media<br>\n",
    "Topic 5 = internet<br>\n",
    "Topic 6 = non_profit<br>\n",
    "Topic 7 = engineering<br>\n",
    "Topic 8 = law<br>\n",
    "Topic 9 = publishing<br>\n",
    "Topic 10 = science<br>\n",
    "Topic 11 = government<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef91185a",
   "metadata": {},
   "source": [
    "# OCCUPATIONS TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3802ab52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 12350)\n"
     ]
    }
   ],
   "source": [
    "#Using the TFIDF Vectorizer to get TFIDF vectors with custom tokeniser. This code was copied from lecture 2.2\n",
    "#it tokenises all the words from each chapter into TF-IDF vectors as explained in the critical report\n",
    "\n",
    "tfidf_vectoriser = TfidfVectorizer(tokenizer=my_tokeniser)\n",
    "tfidf = tfidf_vectoriser.fit_transform(topics_occupation)#(chapters)\n",
    "print(tfidf.todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f90e061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topics occupation 0\n",
      "\"           0.387343\n",
      "'           0.264804\n",
      "wa          0.258826\n",
      "baka:       0.241516\n",
      "kuronue:    0.183737\n",
      "jack        0.180593\n",
      "&nbsp       0.163522\n",
      "captain     0.162937\n",
      "like        0.154818\n",
      "jenn:       0.150226\n",
      "baka        0.146759\n",
      "just        0.144058\n",
      "know        0.139874\n",
      "sparrow:    0.139825\n",
      "really      0.120746\n",
      "i'm         0.119550\n",
      "think       0.111780\n",
      "don't       0.104607\n",
      "demon       0.096265\n",
      "it's        0.092652\n",
      "Name: 0, dtype: float64\n",
      "topics occupation 1\n",
      "\"          0.375646\n",
      "wa         0.366483\n",
      "&nbsp      0.216356\n",
      "like       0.171026\n",
      "time       0.167972\n",
      "credit     0.144731\n",
      "just       0.128269\n",
      "'          0.128269\n",
      "rita       0.123986\n",
      "urllink    0.116499\n",
      "know       0.116053\n",
      "don't      0.116053\n",
      "ha         0.112999\n",
      "day        0.088567\n",
      "good       0.085513\n",
      "iraq       0.085102\n",
      "bush       0.084426\n",
      "way        0.079405\n",
      "think      0.076351\n",
      "year       0.073297\n",
      "Name: 1, dtype: float64\n",
      "topics occupation 2\n",
      "\"          0.487365\n",
      "urllink    0.328362\n",
      "wa         0.228176\n",
      "&rarr      0.171306\n",
      "'          0.168363\n",
      "just       0.143994\n",
      "work       0.139564\n",
      "it's       0.132918\n",
      "like       0.130702\n",
      "time       0.115195\n",
      "video      0.113705\n",
      "35mph      0.107066\n",
      "film       0.096577\n",
      "night      0.090827\n",
      "think      0.090827\n",
      ")          0.088612\n",
      "good       0.088612\n",
      "i'm        0.086397\n",
      "know       0.084181\n",
      "going      0.079751\n",
      "Name: 2, dtype: float64\n",
      "topics occupation 3\n",
      "wa        0.328646\n",
      "just      0.211794\n",
      "like      0.211794\n",
      "time      0.211794\n",
      "know      0.186233\n",
      "i’m       0.169755\n",
      "\"         0.164323\n",
      "&nbsp     0.155215\n",
      "think     0.146065\n",
      "told      0.139402\n",
      "want      0.135315\n",
      "really    0.127807\n",
      "got       0.120504\n",
      "'         0.120504\n",
      "thing     0.120504\n",
      "feel      0.116852\n",
      "today     0.109549\n",
      "mama      0.096399\n",
      "ha        0.094942\n",
      "say       0.091291\n",
      "Name: 3, dtype: float64\n",
      "topics occupation 4\n",
      "soccer      0.328067\n",
      "like        0.237925\n",
      "betting     0.187102\n",
      "story       0.178103\n",
      "bet         0.178103\n",
      "guy         0.169371\n",
      "just        0.133077\n",
      "scene       0.127459\n",
      "think       0.125012\n",
      "film        0.118668\n",
      "idea        0.116946\n",
      "underdog    0.116939\n",
      "bookie      0.116939\n",
      "2004        0.116788\n",
      "simon       0.113819\n",
      "roommate    0.109143\n",
      "\"           0.104848\n",
      "work        0.096783\n",
      "talk        0.096693\n",
      "ok          0.096693\n",
      "Name: 4, dtype: float64\n",
      "topics occupation 5\n",
      "\"            0.411084\n",
      "wa           0.258831\n",
      "i'll         0.232315\n",
      "i'm          0.213155\n",
      "jay          0.202225\n",
      "just         0.152253\n",
      "good         0.152253\n",
      "think        0.137028\n",
      "weekend      0.121803\n",
      "know         0.121803\n",
      "went         0.106577\n",
      "going        0.106577\n",
      "madrid       0.103018\n",
      "liverpool    0.103018\n",
      "colin        0.103018\n",
      "urllink      0.099564\n",
      "'            0.098965\n",
      "got          0.098965\n",
      "i've         0.091352\n",
      "lake         0.088473\n",
      "Name: 5, dtype: float64\n",
      "topics occupation 6\n",
      "'          0.393691\n",
      ":          0.366067\n",
      "wa         0.299955\n",
      "\"          0.212468\n",
      "urllink    0.194107\n",
      "hussam     0.169131\n",
      "jp         0.157050\n",
      "jr         0.140064\n",
      "u          0.121857\n",
      "people     0.115608\n",
      "said       0.109359\n",
      "'i         0.106941\n",
      "officer    0.103751\n",
      "boston     0.102686\n",
      "police     0.098563\n",
      "cw         0.090606\n",
      "ha         0.084362\n",
      "bush       0.078149\n",
      "time       0.078113\n",
      "like       0.074989\n",
      "Name: 6, dtype: float64\n",
      "topics occupation 7\n",
      "[]          1.0\n",
      "\"           0.0\n",
      "ounce       0.0\n",
      "ot          0.0\n",
      "othe        0.0\n",
      "other's     0.0\n",
      "other)      0.0\n",
      "others)     0.0\n",
      "others’     0.0\n",
      "otter       0.0\n",
      "ouch        0.0\n",
      "ought       0.0\n",
      "our&nbsp    0.0\n",
      "orb         0.0\n",
      "ours)       0.0\n",
      "ourself     0.0\n",
      "out&nbsp    0.0\n",
      "out'        0.0\n",
      "out)        0.0\n",
      "out*        0.0\n",
      "Name: 7, dtype: float64\n",
      "topics occupation 8\n",
      "[]          1.0\n",
      "\"           0.0\n",
      "ounce       0.0\n",
      "ot          0.0\n",
      "othe        0.0\n",
      "other's     0.0\n",
      "other)      0.0\n",
      "others)     0.0\n",
      "others’     0.0\n",
      "otter       0.0\n",
      "ouch        0.0\n",
      "ought       0.0\n",
      "our&nbsp    0.0\n",
      "orb         0.0\n",
      "ours)       0.0\n",
      "ourself     0.0\n",
      "out&nbsp    0.0\n",
      "out'        0.0\n",
      "out)        0.0\n",
      "out*        0.0\n",
      "Name: 8, dtype: float64\n",
      "topics occupation 9\n",
      "[]          1.0\n",
      "\"           0.0\n",
      "ounce       0.0\n",
      "ot          0.0\n",
      "othe        0.0\n",
      "other's     0.0\n",
      "other)      0.0\n",
      "others)     0.0\n",
      "others’     0.0\n",
      "otter       0.0\n",
      "ouch        0.0\n",
      "ought       0.0\n",
      "our&nbsp    0.0\n",
      "orb         0.0\n",
      "ours)       0.0\n",
      "ourself     0.0\n",
      "out&nbsp    0.0\n",
      "out'        0.0\n",
      "out)        0.0\n",
      "out*        0.0\n",
      "Name: 9, dtype: float64\n",
      "topics occupation 10\n",
      "[]          1.0\n",
      "\"           0.0\n",
      "ounce       0.0\n",
      "ot          0.0\n",
      "othe        0.0\n",
      "other's     0.0\n",
      "other)      0.0\n",
      "others)     0.0\n",
      "others’     0.0\n",
      "otter       0.0\n",
      "ouch        0.0\n",
      "ought       0.0\n",
      "our&nbsp    0.0\n",
      "orb         0.0\n",
      "ours)       0.0\n",
      "ourself     0.0\n",
      "out&nbsp    0.0\n",
      "out'        0.0\n",
      "out)        0.0\n",
      "out*        0.0\n",
      "Name: 10, dtype: float64\n",
      "topics occupation 11\n",
      "[]          1.0\n",
      "\"           0.0\n",
      "ounce       0.0\n",
      "ot          0.0\n",
      "othe        0.0\n",
      "other's     0.0\n",
      "other)      0.0\n",
      "others)     0.0\n",
      "others’     0.0\n",
      "otter       0.0\n",
      "ouch        0.0\n",
      "ought       0.0\n",
      "our&nbsp    0.0\n",
      "orb         0.0\n",
      "ours)       0.0\n",
      "ourself     0.0\n",
      "out&nbsp    0.0\n",
      "out'        0.0\n",
      "out)        0.0\n",
      "out*        0.0\n",
      "Name: 11, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#This code was copied from lecture 2.2. It displays the TF-IDF weight values for the least common vectors for\n",
    "#each document according to occupation.\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf.todense(), columns = vocab)\n",
    "for i in range(len(tfidf_df)):\n",
    "    print(\"topics occupation\", i)\n",
    "    print(tfidf_df.iloc[i].sort_values(ascending = False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430762d1",
   "metadata": {},
   "source": [
    "Topic 0 = student<br>\n",
    "Topic 1 = technology<br>\n",
    "Topic 2 = arts<br>\n",
    "Topic 3 = education<br>\n",
    "Topic 4 = communications_media<br>\n",
    "Topic 5 = internet<br>\n",
    "Topic 6 = non_profit<br>\n",
    "Topic 7 = engineering<br>\n",
    "Topic 8 = law<br>\n",
    "Topic 9 = publishing<br>\n",
    "Topic 10 = science<br>\n",
    "Topic 11 = government<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e278f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
